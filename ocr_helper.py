"""
OCR Helper Class - åŸºäºPaddleOCRçš„æ–‡å­—è¯†åˆ«å’Œå®šä½å·¥å…·ç±»
æä¾›ç»Ÿä¸€çš„æ¥å£ä¾›å¤–éƒ¨è°ƒç”¨ï¼Œè¾“å…¥å›¾åƒæ–‡ä»¶å’Œè¦æŸ¥æ‰¾çš„æ–‡å­—ï¼Œè¿”å›æ–‡å­—æ‰€åœ¨çš„å›¾ç‰‡åŒºåŸŸ
æ”¯æŒåŒºåŸŸåˆ†å‰²åŠŸèƒ½ï¼Œå¯ä»¥æŒ‡å®šåªè¯†åˆ«å±å¹•çš„ç‰¹å®šåŒºåŸŸï¼Œå¤§å¤§æå‡è¯†åˆ«é€Ÿåº¦
"""

from paddleocr import PaddleOCR
import json
import os
import time
import cv2
from airtest.core.api import snapshot, touch
from airtest.aircv.cal_confidence import cal_ccoeff_confidence
import logging
import coloredlogs
from typing import List, Tuple, Optional, Dict, Any


class OCRHelper:
    """OCRè¾…åŠ©å·¥å…·ç±»ï¼Œå°è£…PaddleOCRåŠŸèƒ½"""

    def __init__(
        self,
        output_dir="output",
        use_doc_orientation_classify=False,
        use_doc_unwarping=False,
        use_textline_orientation=False,
        resize_image=True,
        max_width=960,
    ):
        """
        åˆå§‹åŒ–OCR Helper

        Args:
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
            use_doc_orientation_classify (bool): æ˜¯å¦ä½¿ç”¨æ–‡æ¡£æ–¹å‘åˆ†ç±»æ¨¡å‹
            use_doc_unwarping (bool): æ˜¯å¦ä½¿ç”¨æ–‡æœ¬å›¾åƒçŸ«æ­£æ¨¡å‹
            use_textline_orientation (bool): æ˜¯å¦ä½¿ç”¨æ–‡æœ¬è¡Œæ–¹å‘åˆ†ç±»æ¨¡å‹
            resize_image (bool): æ˜¯å¦è‡ªåŠ¨ç¼©å°å›¾ç‰‡ä»¥æå‡é€Ÿåº¦
            max_width (int): å›¾ç‰‡æœ€å¤§å®½åº¦ï¼Œé»˜è®¤960ï¼ˆå»ºè®®åœ¨640-960ä¹‹é—´ï¼‰
        """
        self.output_dir = output_dir
        self.resize_image = resize_image
        self.max_width = max_width

        self.ocr = PaddleOCR(
            use_doc_orientation_classify=use_doc_orientation_classify,
            use_doc_unwarping=use_doc_unwarping,
            use_textline_orientation=use_textline_orientation,
            text_detection_model_name="PP-OCRv5_mobile_det",
            text_recognition_model_name="PP-OCRv5_mobile_rec",
            cpu_threads=4,  # M4 æ•ˆç‡æ ¸å¿ƒï¼Œå‡å°‘çº¿ç¨‹ç«äº‰
        )

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        # é…ç½®å½©è‰²æ—¥å¿—ï¼ˆéœ€è¦å…ˆåˆå§‹åŒ–ï¼Œå› ä¸ºç¼“å­˜åŠ è½½æ—¶ä¼šç”¨åˆ°ï¼‰
        self.logger = logging.getLogger(f"{__name__}.OCRHelper")
        # é˜²æ­¢æ—¥å¿—é‡å¤ï¼šç§»é™¤å·²æœ‰çš„ handlers
        self.logger.handlers.clear()
        self.logger.propagate = False

        coloredlogs.install(
            level="INFO",
            logger=self.logger,
            fmt="%(asctime)s %(name)s %(levelname)s %(message)s",
            datefmt="%H:%M:%S",
            level_styles={
                "debug": {"color": "cyan"},
                "info": {"color": "blue"},
                "warning": {"color": "yellow"},
                "error": {"color": "red"},
                "critical": {"color": "red", "bold": True},
            },
        )

        # åˆå§‹åŒ–ç¼“å­˜
        # æ ¼å¼: [(image_path, json_file_path), ...]
        self.ocr_cache = []
        self.cache_dir = os.path.join(self.output_dir, "cache")
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

        # ç¼“å­˜ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ95%ä»¥ä¸Šè®¤ä¸ºæ˜¯åŒä¸€å¼ å›¾ï¼‰
        self.cache_similarity_threshold = 0.95

        # åŠ è½½å·²æœ‰çš„ç¼“å­˜ï¼ˆéœ€è¦åœ¨ logger åˆå§‹åŒ–ä¹‹åï¼‰
        self._load_existing_cache()

    def _merge_regions(self, regions: List[int]) -> Tuple[int, int, int, int]:
        """
        åˆå¹¶å¤šä¸ªåŒºåŸŸä¸ºä¸€ä¸ªè¿ç»­çš„çŸ©å½¢åŒºåŸŸ

        Args:
            regions: è¦åˆå¹¶çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰
                    1 2 3
                    4 5 6
                    7 8 9

        Returns:
            åˆå¹¶åçš„è¾¹ç•Œ (min_row, max_row, min_col, max_col)ï¼Œéƒ½æ˜¯0-basedç´¢å¼•
        """
        if not regions:
            return (0, 2, 0, 2)  # æ•´ä¸ªå›¾åƒ

        # å°†åŒºåŸŸIDè½¬æ¢ä¸ºè¡Œåˆ—ç´¢å¼•
        rows = []
        cols = []
        for region_id in regions:
            if not 1 <= region_id <= 9:
                self.logger.warning(f"æ— æ•ˆçš„åŒºåŸŸID: {region_id}ï¼Œè·³è¿‡")
                continue
            row = (region_id - 1) // 3
            col = (region_id - 1) % 3
            rows.append(row)
            cols.append(col)

        if not rows:
            return (0, 2, 0, 2)

        # è®¡ç®—åŒ…å«æ‰€æœ‰åŒºåŸŸçš„æœ€å°çŸ©å½¢
        min_row = min(rows)
        max_row = max(rows)
        min_col = min(cols)
        max_col = max(cols)

        return (min_row, max_row, min_col, max_col)

    def _get_region_bounds(
        self, image_shape: Tuple[int, int], regions: Optional[List[int]] = None
    ) -> Tuple[int, int, int, int]:
        """
        å°†å›¾åƒåˆ†æˆ3x3ç½‘æ ¼ï¼Œè¿”å›åˆå¹¶åçš„åŒºåŸŸè¾¹ç•Œ

        Args:
            image_shape: å›¾åƒå½¢çŠ¶ (height, width)
            regions: è¦æå–çš„åŒºåŸŸåˆ—è¡¨ï¼Œä½¿ç”¨æ•°å­—1-9è¡¨ç¤ºï¼ˆä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹ï¼‰
                    1 2 3
                    4 5 6
                    7 8 9
                    å¦‚æœä¸ºNoneï¼Œè¿”å›æ•´ä¸ªå›¾åƒ
                    å¤šä¸ªåŒºåŸŸä¼šè¢«åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„çŸ©å½¢

        Returns:
            åŒºåŸŸè¾¹ç•Œ (x, y, w, h)
        """
        height, width = image_shape

        if regions is None:
            # è¿”å›æ•´ä¸ªå›¾åƒ
            return (0, 0, width, height)

        # åˆå¹¶åŒºåŸŸ
        min_row, max_row, min_col, max_col = self._merge_regions(regions)

        # è®¡ç®—æ¯ä¸ªæ ¼å­çš„å¤§å°
        cell_height = height // 3
        cell_width = width // 3

        # è®¡ç®—åˆå¹¶åçš„è¾¹ç•Œ
        x = min_col * cell_width
        y = min_row * cell_height
        w = (max_col - min_col + 1) * cell_width
        h = (max_row - min_row + 1) * cell_height

        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œç¡®ä¿è¦†ç›–åˆ°å›¾åƒè¾¹ç¼˜
        if max_col == 2:  # åŒ…å«æœ€å³åˆ—
            w = width - x
        if max_row == 2:  # åŒ…å«æœ€ä¸‹è¡Œ
            h = height - y

        return (x, y, w, h)

    def _extract_region(
        self,
        image: Any,
        regions: Optional[List[int]] = None,
        debug_save_path: Optional[str] = None,
    ) -> Tuple[Any, Tuple[int, int]]:
        """
        ä»å›¾åƒä¸­æå–æŒ‡å®šçš„åŒºåŸŸï¼ˆåˆå¹¶åçš„å•ä¸ªåŒºåŸŸï¼‰

        Args:
            image: OpenCVå›¾åƒå¯¹è±¡
            regions: è¦æå–çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œä¼šè¢«åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„çŸ©å½¢
            debug_save_path: è°ƒè¯•ç”¨ï¼Œä¿å­˜åŒºåŸŸæˆªå›¾çš„è·¯å¾„

        Returns:
            (region_image, (offset_x, offset_y))
        """
        if image is None:
            return None, (0, 0)

        height, width = image.shape[:2]
        x, y, w, h = self._get_region_bounds((height, width), regions)

        region_img = image[y : y + h, x : x + w]

        # è°ƒè¯•ï¼šä¿å­˜åŒºåŸŸæˆªå›¾
        if debug_save_path:
            import cv2

            cv2.imwrite(debug_save_path, region_img)
            self.logger.info(f"ğŸ” è°ƒè¯•ï¼šåŒºåŸŸæˆªå›¾å·²ä¿å­˜åˆ° {debug_save_path}")
            self.logger.info(f"   åŒºåŸŸèŒƒå›´: x={x}, y={y}, w={w}, h={h}")
            self.logger.info(f"   åŸå›¾å°ºå¯¸: {width}x{height}")

        return region_img, (x, y)

    def _adjust_coordinates_to_full_image(
        self, bbox: List[List[int]], offset: Tuple[int, int]
    ) -> List[List[int]]:
        """
        å°†åŒºåŸŸå†…çš„åæ ‡è°ƒæ•´ä¸ºåŸå›¾ä¸­çš„åæ ‡

        Args:
            bbox: åŒºåŸŸå†…çš„è¾¹ç•Œæ¡†åæ ‡ [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
            offset: åŒºåŸŸåœ¨åŸå›¾ä¸­çš„åç§»é‡ (offset_x, offset_y)

        Returns:
            è°ƒæ•´åçš„è¾¹ç•Œæ¡†åæ ‡
        """
        offset_x, offset_y = offset
        adjusted_bbox = []
        for point in bbox:
            adjusted_bbox.append([point[0] + offset_x, point[1] + offset_y])
        return adjusted_bbox

    def _load_existing_cache(self):
        """
        åŠ è½½ç¼“å­˜ç›®å½•ä¸­å·²æœ‰çš„ç¼“å­˜æ–‡ä»¶
        """
        try:
            if not os.path.exists(self.cache_dir):
                return

            # æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜æ–‡ä»¶å¯¹
            cache_files = os.listdir(self.cache_dir)
            cache_pairs = {}

            # å°†å›¾ç‰‡å’Œ JSON æ–‡ä»¶é…å¯¹
            for filename in cache_files:
                if filename.startswith("cache_") and filename.endswith(".png"):
                    # æå–ç¼“å­˜ ID
                    cache_id = filename.replace("cache_", "").replace(".png", "")
                    json_filename = f"cache_{cache_id}_res.json"

                    image_path = os.path.join(self.cache_dir, filename)
                    json_path = os.path.join(self.cache_dir, json_filename)

                    # æ£€æŸ¥å¯¹åº”çš„ JSON æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if os.path.exists(json_path):
                        cache_pairs[cache_id] = (image_path, json_path)

            # æŒ‰ ID æ’åºå¹¶åŠ è½½åˆ°ç¼“å­˜åˆ—è¡¨
            for cache_id in sorted(
                cache_pairs.keys(), key=lambda x: int(x) if x.isdigit() else 0
            ):
                self.ocr_cache.append(cache_pairs[cache_id])

            if self.ocr_cache:
                self.logger.info(f"ğŸ’¾ åŠ è½½äº† {len(self.ocr_cache)} ä¸ªç¼“å­˜æ–‡ä»¶")
        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

    def _find_similar_cached_image(self, current_image_path):
        """
        æŸ¥æ‰¾ç¼“å­˜ä¸­æ˜¯å¦æœ‰ç›¸ä¼¼çš„å›¾ç‰‡

        Args:
            current_image_path (str): å½“å‰å›¾ç‰‡è·¯å¾„

        Returns:
            str: ç¼“å­˜çš„ JSON æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        try:
            current_img = cv2.imread(current_image_path)
            if current_img is None:
                return None

            # éå†ç¼“å­˜ï¼ŒæŸ¥æ‰¾ç›¸ä¼¼å›¾ç‰‡
            for cached_img_path, cached_json_path in self.ocr_cache:
                if not os.path.exists(cached_img_path) or not os.path.exists(
                    cached_json_path
                ):
                    continue

                cached_img = cv2.imread(cached_img_path)
                if cached_img is None:
                    continue

                # è°ƒæ•´å›¾ç‰‡å°ºå¯¸ä¸€è‡´ä»¥ä¾¿æ¯”è¾ƒ
                if current_img.shape != cached_img.shape:
                    cached_img = cv2.resize(
                        cached_img, (current_img.shape[1], current_img.shape[0])
                    )

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = cal_ccoeff_confidence(current_img, cached_img)

                if similarity >= self.cache_similarity_threshold:
                    self.logger.info(
                        f"ğŸ’¾ æ‰¾åˆ°ç›¸ä¼¼ç¼“å­˜å›¾ç‰‡ (ç›¸ä¼¼åº¦: {similarity * 100:.1f}%)"
                    )
                    return cached_json_path

            return None
        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼ç¼“å­˜å›¾ç‰‡å¤±è´¥: {e}")
            return None

    def _save_to_cache(self, image_path, json_file):
        """
        ä¿å­˜å›¾ç‰‡å’Œ OCR ç»“æœåˆ°ç¼“å­˜

        Args:
            image_path (str): å›¾ç‰‡è·¯å¾„
            json_file (str): JSON æ–‡ä»¶è·¯å¾„
        """
        try:
            import shutil

            # ä¸ºç¼“å­˜åˆ›å»ºå”¯ä¸€çš„æ–‡ä»¶å
            cache_id = len(self.ocr_cache)
            cache_image_name = f"cache_{cache_id}.png"
            cache_json_name = f"cache_{cache_id}_res.json"

            cache_image_path = os.path.join(self.cache_dir, cache_image_name)
            cache_json_path = os.path.join(self.cache_dir, cache_json_name)

            # å¤åˆ¶å›¾ç‰‡åˆ°ç¼“å­˜ç›®å½•
            shutil.copy2(image_path, cache_image_path)

            # å¤åˆ¶ JSON åˆ°ç¼“å­˜ç›®å½•
            if os.path.exists(json_file):
                shutil.copy2(json_file, cache_json_path)
                # ä¿å­˜ç¼“å­˜è®°å½•
                self.ocr_cache.append((cache_image_path, cache_json_path))
                self.logger.debug(
                    f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ (å›¾ç‰‡æ•°: {len(self.ocr_cache)}, JSON: {cache_json_name})"
                )
            else:
                self.logger.error(f"JSON æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ç¼“å­˜: {json_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _resize_image_for_ocr(self, image_path):
        """
        è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥åŠ é€Ÿ OCR è¯†åˆ«

        Args:
            image_path (str): åŸå§‹å›¾ç‰‡è·¯å¾„

        Returns:
            str: è°ƒæ•´åçš„å›¾ç‰‡è·¯å¾„ï¼ˆå¦‚æœéœ€è¦è°ƒæ•´ï¼‰ï¼Œå¦åˆ™è¿”å›åŸè·¯å¾„
        """
        if not self.resize_image:
            return image_path

        try:
            img = cv2.imread(image_path)
            if img is None:
                return image_path

            height, width = img.shape[:2]

            # å¦‚æœå›¾ç‰‡å®½åº¦å¤§äºæœ€å¤§å®½åº¦ï¼Œè¿›è¡Œç¼©æ”¾
            if width > self.max_width:
                scale = self.max_width / width
                new_width = self.max_width
                new_height = int(height * scale)

                # ç¼©å°å›¾ç‰‡
                resized_img = cv2.resize(
                    img, (new_width, new_height), interpolation=cv2.INTER_AREA
                )

                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_path = image_path.replace(".png", "_resized.png")
                cv2.imwrite(temp_path, resized_img)

                self.logger.debug(
                    f"ğŸ”§ å›¾ç‰‡å·²ç¼©å°: {width}x{height} -> {new_width}x{new_height}"
                )
                return temp_path

            return image_path
        except Exception as e:
            self.logger.warning(f"å›¾ç‰‡ç¼©æ”¾å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå›¾")
            return image_path

    def _predict_with_timing(self, image_path):
        """
        æ‰§è¡Œ OCR è¯†åˆ«å¹¶è®°å½•è€—æ—¶

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„

        Returns:
            OCR è¯†åˆ«ç»“æœ
        """
        # é¢„å¤„ç†ï¼šç¼©å°å›¾ç‰‡
        processed_image_path = self._resize_image_for_ocr(image_path)

        start_time = time.time()
        result = self.ocr.predict(processed_image_path)
        elapsed_time = time.time() - start_time

        filename = os.path.basename(image_path)
        self.logger.info(f"â±ï¸ OCRè¯†åˆ«è€—æ—¶: {elapsed_time:.3f}ç§’ (æ–‡ä»¶: {filename})")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if processed_image_path != image_path and os.path.exists(processed_image_path):
            try:
                os.remove(processed_image_path)
            except Exception:
                pass

        return result

    def _get_or_create_ocr_result(self, image_path, use_cache=True):
        """
        è·å–æˆ–åˆ›å»º OCR è¯†åˆ«ç»“æœï¼ˆå¸¦ç¼“å­˜ï¼‰

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True

        Returns:
            str: JSON æ–‡ä»¶è·¯å¾„
        """
        # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œæ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰ç›¸ä¼¼å›¾ç‰‡
        if use_cache:
            cached_json = self._find_similar_cached_image(image_path)
            if cached_json:
                return cached_json

        # ç¼“å­˜æœªå‘½ä¸­æˆ–ç¦ç”¨ç¼“å­˜ï¼Œæ‰§è¡Œ OCR è¯†åˆ«
        result = self._predict_with_timing(image_path)

        if result and len(result) > 0:
            # å…ˆä¿å­˜åˆ° output ç›®å½•ï¼ˆæ ‡å‡†æµç¨‹ï¼‰
            for res in result:
                res.save_to_json(self.output_dir)

            # è·å– JSON æ–‡ä»¶è·¯å¾„
            json_file = os.path.join(
                self.output_dir,
                os.path.basename(image_path).replace(".png", "_res.json"),
            )

            # å¦‚æœå¯ç”¨ç¼“å­˜ï¼ŒåŒæ—¶ä¿å­˜åˆ°ç¼“å­˜
            if use_cache and os.path.exists(json_file):
                self._save_to_cache(image_path, json_file)

            return json_file

        return None

    def find_text_in_image(
        self,
        image_path,
        target_text,
        confidence_threshold=0.5,
        occurrence=1,
        use_cache=True,
        regions: Optional[List[int]] = None,
        debug_save_path: Optional[str] = None,
    ):
        """
        åœ¨æŒ‡å®šå›¾åƒä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—çš„ä½ç½®

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            occurrence (int): æŒ‡å®šç‚¹å‡»ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æœç´¢æ•´ä¸ªå›¾åƒ
                                          åŒºåŸŸç¼–å·å¦‚ä¸‹ï¼š
                                          1 2 3
                                          4 5 6
                                          7 8 9
            debug_save_path (str, optional): è°ƒè¯•ç”¨ï¼Œä¿å­˜åŒºåŸŸæˆªå›¾çš„è·¯å¾„

        Returns:
            dict: åŒ…å«ä»¥ä¸‹ä¿¡æ¯çš„å­—å…¸
                - found (bool): æ˜¯å¦æ‰¾åˆ°æ–‡å­—
                - center (tuple): æ–‡å­—ä¸­å¿ƒåæ ‡ (x, y)ï¼Œæœªæ‰¾åˆ°æ—¶ä¸ºNone
                - text (str): å®é™…è¯†åˆ«åˆ°çš„æ–‡å­—ï¼Œæœªæ‰¾åˆ°æ—¶ä¸ºNone
                - confidence (float): ç½®ä¿¡åº¦ï¼Œæœªæ‰¾åˆ°æ—¶ä¸ºNone
                - bbox (list): æ–‡å­—è¾¹ç•Œæ¡†åæ ‡ï¼Œæœªæ‰¾åˆ°æ—¶ä¸ºNone
                - total_matches (int): æ€»å…±æ‰¾åˆ°çš„åŒ¹é…æ•°é‡
                - selected_index (int): å®é™…é€‰æ‹©çš„ç´¢å¼• (1-based)
        """
        try:
            # å¦‚æœæŒ‡å®šäº†åŒºåŸŸï¼Œä½¿ç”¨åŒºåŸŸæœç´¢
            if regions is not None:
                return self._find_text_in_regions(
                    image_path,
                    target_text,
                    confidence_threshold,
                    occurrence,
                    regions,
                    debug_save_path,
                )

            # è·å–æˆ–åˆ›å»º OCR ç»“æœï¼ˆå¸¦ç¼“å­˜ï¼‰
            json_file = self._get_or_create_ocr_result(image_path, use_cache=use_cache)

            if not json_file:
                self.logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return {
                    "found": False,
                    "center": None,
                    "text": None,
                    "confidence": None,
                    "bbox": None,
                    "total_matches": 0,
                    "selected_index": 0,
                }

            # ä» JSON ä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—
            return self._find_text_in_json(
                json_file, target_text, confidence_threshold, occurrence
            )

        except Exception as e:
            self.logger.error(f"å›¾åƒOCRè¯†åˆ«å‡ºé”™: {e}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }

    def _find_text_in_regions(
        self,
        image_path: str,
        target_text: str,
        confidence_threshold: float,
        occurrence: int,
        regions: List[int],
        debug_save_path: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        åœ¨æŒ‡å®šåŒºåŸŸä¸­æŸ¥æ‰¾æ–‡å­—ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            target_text: è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            occurrence: æŒ‡å®šç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­—
            regions: è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆä¼šè¢«åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„çŸ©å½¢ï¼‰
            debug_save_path: è°ƒè¯•ç”¨ï¼Œä¿å­˜åŒºåŸŸæˆªå›¾çš„è·¯å¾„

        Returns:
            æŸ¥æ‰¾ç»“æœå­—å…¸
        """
        try:
            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            if image is None:
                self.logger.error(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
                return self._empty_result()

            # æå–åˆå¹¶åçš„åŒºåŸŸ
            region_img, offset = self._extract_region(image, regions, debug_save_path)
            if region_img is None:
                self.logger.warning("æœªèƒ½æå–åŒºåŸŸ")
                return self._empty_result()

            # æ˜¾ç¤ºåŒºåŸŸä¿¡æ¯
            region_desc = self._get_region_description(regions)
            self.logger.info(f"ğŸ” åœ¨{region_desc}æœç´¢æ–‡å­—: '{target_text}'")

            # å¯¹åŒºåŸŸè¿›è¡ŒOCRè¯†åˆ«
            start_time = time.time()
            result = self.ocr.predict(region_img)
            elapsed_time = time.time() - start_time
            self.logger.info(f"â±ï¸ åŒºåŸŸ OCR è€—æ—¶: {elapsed_time:.3f}ç§’ (åç§»: {offset})")

            if not result or len(result) == 0:
                self.logger.warning("OCRè¯†åˆ«ç»“æœä¸ºç©º")
                return self._empty_result()

            # æ”¶é›†æ‰€æœ‰åŒ¹é…ç»“æœ
            all_matches = []
            for res in result:
                # æ”¯æŒä¸¤ç§è®¿é—®æ–¹å¼ï¼šå±æ€§è®¿é—®å’Œå­—å…¸è®¿é—®
                if hasattr(res, "rec_texts"):
                    rec_texts = res.rec_texts
                    rec_scores = res.rec_scores
                    dt_polys = res.dt_polys
                elif isinstance(res, dict):
                    rec_texts = res.get("rec_texts", [])
                    rec_scores = res.get("rec_scores", [])
                    dt_polys = res.get("dt_polys", [])
                else:
                    # å°è¯•ä½œä¸ºå­—å…¸è®¿é—®ï¼ˆOCRResult å¯¹è±¡ï¼‰
                    try:
                        rec_texts = res["rec_texts"]
                        rec_scores = res["rec_scores"]
                        dt_polys = res["dt_polys"]
                    except (KeyError, TypeError):
                        rec_texts = []
                        rec_scores = []
                        dt_polys = []

                # æŸ¥æ‰¾åŒ¹é…çš„æ–‡å­—
                for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                    if score >= confidence_threshold and target_text in text:
                        if i < len(dt_polys):
                            poly = dt_polys[i]

                            # è°ƒæ•´åæ ‡åˆ°åŸå›¾
                            adjusted_poly = self._adjust_coordinates_to_full_image(
                                poly, offset
                            )

                            # è®¡ç®—ä¸­å¿ƒç‚¹
                            x_coords = [point[0] for point in adjusted_poly]
                            y_coords = [point[1] for point in adjusted_poly]
                            center_x = int(sum(x_coords) / len(x_coords))
                            center_y = int(sum(y_coords) / len(y_coords))

                            all_matches.append(
                                {
                                    "center": (center_x, center_y),
                                    "text": text,
                                    "confidence": score,
                                    "bbox": adjusted_poly,
                                    "index": len(all_matches) + 1,
                                }
                            )

            # å¤„ç†åŒ¹é…ç»“æœ
            total_matches = len(all_matches)
            self.logger.info(f"æ‰¾åˆ° {total_matches} ä¸ªåŒ¹é…çš„æ–‡å­—")

            if total_matches == 0:
                return self._empty_result()

            # æ˜¾ç¤ºæ‰€æœ‰åŒ¹é…
            for i, match in enumerate(all_matches, 1):
                self.logger.info(
                    f"  åŒ¹é… {i}: '{match['text']}' (ç½®ä¿¡åº¦: {match['confidence']:.3f}) ä½ç½®: {match['center']}"
                )

            # é€‰æ‹©æŒ‡å®šçš„åŒ¹é…é¡¹
            if occurrence > total_matches:
                self.logger.warning(
                    f"è¯·æ±‚ç¬¬{occurrence}ä¸ªåŒ¹é…ï¼Œä½†åªæ‰¾åˆ°{total_matches}ä¸ªï¼Œä½¿ç”¨æœ€åä¸€ä¸ª"
                )
                selected_match = all_matches[-1]
                selected_index = total_matches
            else:
                selected_match = all_matches[occurrence - 1]
                selected_index = occurrence

            self.logger.info(
                f"é€‰æ‹©ç¬¬{selected_index}ä¸ªåŒ¹é…: '{selected_match['text']}'"
            )

            return {
                "found": True,
                "center": selected_match["center"],
                "text": selected_match["text"],
                "confidence": selected_match["confidence"],
                "bbox": selected_match["bbox"],
                "total_matches": total_matches,
                "selected_index": selected_index,
            }

        except Exception as e:
            self.logger.error(f"åŒºåŸŸæœç´¢å‡ºé”™: {e}")
            return self._empty_result()

    def _get_region_description(self, regions: Optional[List[int]]) -> str:
        """
        è·å–åŒºåŸŸçš„æè¿°æ–‡å­—

        Args:
            regions: åŒºåŸŸåˆ—è¡¨

        Returns:
            åŒºåŸŸæè¿°ï¼Œå¦‚ "åŒºåŸŸ[1,2,3]ï¼ˆä¸Šéƒ¨ï¼‰"
        """
        if not regions:
            return "å…¨å±"

        # åˆå¹¶åŒºåŸŸ
        min_row, max_row, min_col, max_col = self._merge_regions(regions)

        # ç”Ÿæˆæè¿°
        parts = []

        # è¡Œæè¿°
        if min_row == max_row:
            row_names = ["ä¸Šéƒ¨", "ä¸­éƒ¨", "ä¸‹éƒ¨"]
            parts.append(row_names[min_row])
        elif min_row == 0 and max_row == 2:
            parts.append("å…¨é«˜")
        else:
            parts.append(f"ç¬¬{min_row + 1}-{max_row + 1}è¡Œ")

        # åˆ—æè¿°
        if min_col == max_col:
            col_names = ["å·¦ä¾§", "ä¸­é—´", "å³ä¾§"]
            parts.append(col_names[min_col])
        elif min_col == 0 and max_col == 2:
            parts.append("å…¨å®½")
        else:
            parts.append(f"ç¬¬{min_col + 1}-{max_col + 1}åˆ—")

        region_str = ",".join(map(str, sorted(regions)))
        return f"åŒºåŸŸ[{region_str}]ï¼ˆ{' '.join(parts)}ï¼‰"

    def _empty_result(self) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„æŸ¥æ‰¾ç»“æœ"""
        return {
            "found": False,
            "center": None,
            "text": None,
            "confidence": None,
            "bbox": None,
            "total_matches": 0,
            "selected_index": 0,
        }

    def capture_and_find_text(
        self,
        target_text,
        confidence_threshold=0.5,
        screenshot_path="/tmp/screenshot.png",
        occurrence=1,
        use_cache=True,
        regions: Optional[List[int]] = None,
    ):
        """
        æˆªå›¾å¹¶æŸ¥æ‰¾ç›®æ ‡æ–‡å­—çš„ä½ç½®

        Args:
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            screenshot_path (str): æˆªå›¾ä¿å­˜è·¯å¾„
            occurrence (int): æŒ‡å®šç‚¹å‡»ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æœç´¢æ•´ä¸ªå›¾åƒ

        Returns:
            dict: åŒ…å«æŸ¥æ‰¾ç»“æœçš„å­—å…¸ï¼Œæ ¼å¼åŒfind_text_in_image
        """
        try:
            # æˆªå›¾
            snapshot(filename=screenshot_path)
            self.logger.info(f"æˆªå›¾ä¿å­˜åˆ°: {screenshot_path}")

            # åœ¨æˆªå›¾ä¸­æŸ¥æ‰¾æ–‡å­—
            return self.find_text_in_image(
                screenshot_path,
                target_text,
                confidence_threshold,
                occurrence,
                use_cache,
                regions,
            )

        except Exception as e:
            self.logger.error(f"æˆªå›¾å’Œè¯†åˆ«è¿‡ç¨‹å‡ºé”™: {e}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }

    def find_and_click_text(
        self,
        target_text,
        confidence_threshold=0.5,
        screenshot_path="/tmp/screenshot.png",
        occurrence=1,
        use_cache=True,
        regions: Optional[List[int]] = None,
    ):
        """
        æˆªå›¾ã€æŸ¥æ‰¾æ–‡å­—å¹¶ç‚¹å‡»å…¶ä¸­å¿ƒç‚¹

        Args:
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            screenshot_path (str): æˆªå›¾ä¿å­˜è·¯å¾„
            occurrence (int): æŒ‡å®šç‚¹å‡»ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æœç´¢æ•´ä¸ªå›¾åƒ

        Returns:
            bool: æ˜¯å¦æˆåŠŸæ‰¾åˆ°å¹¶ç‚¹å‡»
        """
        result = self.capture_and_find_text(
            target_text,
            confidence_threshold,
            screenshot_path,
            occurrence,
            use_cache,
            regions,
        )

        if result["found"]:
            center = result["center"]
            self.logger.info(f"ç‚¹å‡»ä½ç½®: {center}")
            touch(center)
            return True
        else:
            self.logger.warning(f"æ— æ³•ç‚¹å‡»ï¼Œæœªæ‰¾åˆ°æ–‡å­—: '{target_text}'")
            return False

    def _find_text_in_json(
        self, json_file_path, target_text, confidence_threshold=0.5, occurrence=1
    ):
        """
        ä»PaddleOCRè¾“å‡ºçš„JSONæ–‡ä»¶ä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—

        Args:
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            occurrence (int): æŒ‡å®šè¿”å›ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1

        Returns:
            dict: æŸ¥æ‰¾ç»“æœå­—å…¸
        """
        try:
            # è¯»å–JSONæ–‡ä»¶
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # è·å–è¯†åˆ«çš„æ–‡å­—åˆ—è¡¨å’Œå¯¹åº”çš„åæ ‡æ¡†
            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])  # æ£€æµ‹æ¡†åæ ‡

            self.logger.info(f"åœ¨JSONä¸­æŸ¥æ‰¾æ–‡å­—: '{target_text}' (ç¬¬{occurrence}ä¸ª)")
            self.logger.info(f"æ€»å…±è¯†åˆ«åˆ° {len(rec_texts)} ä¸ªæ–‡å­—")

            # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ–‡å­—
            matches = []
            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                self.logger.debug(f"  {i + 1:2d}. '{text}' (ç½®ä¿¡åº¦: {score:.3f})")

                # æ£€æŸ¥ç½®ä¿¡åº¦å’Œæ–‡å­—åŒ¹é…ï¼ˆè¯†åˆ«å‡ºçš„æ–‡å­—åŒ…å«ç›®æ ‡æ–‡å­—ï¼‰
                if score >= confidence_threshold and target_text in text:
                    # è·å–å¯¹åº”çš„åæ ‡æ¡†
                    if i < len(dt_polys):
                        poly = dt_polys[i]

                        # è®¡ç®—ä¸­å¿ƒç‚¹åæ ‡
                        # poly æ ¼å¼: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        center_x = int(sum(x_coords) / len(x_coords))
                        center_y = int(sum(y_coords) / len(y_coords))

                        matches.append(
                            {
                                "center": (center_x, center_y),
                                "text": text,
                                "confidence": score,
                                "bbox": poly,
                                "index": len(matches) + 1,
                            }
                        )

            total_matches = len(matches)
            self.logger.info(f"æ‰¾åˆ° {total_matches} ä¸ªåŒ¹é…çš„æ–‡å­—")

            if total_matches == 0:
                self.logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡å­—: '{target_text}'")
                return {
                    "found": False,
                    "center": None,
                    "text": None,
                    "confidence": None,
                    "bbox": None,
                    "total_matches": 0,
                    "selected_index": 0,
                }

            # æ˜¾ç¤ºæ‰€æœ‰åŒ¹é…çš„æ–‡å­—
            for i, match in enumerate(matches, 1):
                self.logger.info(
                    f"  åŒ¹é… {i}: '{match['text']}' (ç½®ä¿¡åº¦: {match['confidence']:.3f}) ä½ç½®: {match['center']}"
                )

            # é€‰æ‹©æŒ‡å®šçš„åŒ¹é…é¡¹
            if occurrence > total_matches:
                self.logger.warning(
                    f"è¯·æ±‚ç¬¬{occurrence}ä¸ªåŒ¹é…ï¼Œä½†åªæ‰¾åˆ°{total_matches}ä¸ªï¼Œä½¿ç”¨æœ€åä¸€ä¸ª"
                )
                selected_match = matches[-1]
                selected_index = total_matches
            else:
                selected_match = matches[occurrence - 1]
                selected_index = occurrence

            self.logger.info(
                f"é€‰æ‹©ç¬¬{selected_index}ä¸ªåŒ¹é…: '{selected_match['text']}'"
            )
            self.logger.info(f"åæ ‡æ¡†: {selected_match['bbox']}")
            self.logger.info(f"ä¸­å¿ƒç‚¹: {selected_match['center']}")

            return {
                "found": True,
                "center": selected_match["center"],
                "text": selected_match["text"],
                "confidence": selected_match["confidence"],
                "bbox": selected_match["bbox"],
                "total_matches": total_matches,
                "selected_index": selected_index,
            }

        except FileNotFoundError:
            self.logger.error(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }
        except json.JSONDecodeError:
            self.logger.error(f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {json_file_path}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }
        except Exception as e:
            self.logger.error(f"å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }

    def find_all_matching_texts(
        self, image_path, target_text, confidence_threshold=0.5
    ):
        """
        æŸ¥æ‰¾å›¾åƒä¸­æ‰€æœ‰åŒ¹é…çš„æ–‡å­—

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)

        Returns:
            list: åŒ…å«æ‰€æœ‰åŒ¹é…æ–‡å­—ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«center, text, confidence, bbox
        """
        try:
            # OCR è¯†åˆ«
            result = self._predict_with_timing(image_path)

            if not result or len(result) == 0:
                self.logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return []

            # ä¿å­˜è¯†åˆ«ç»“æœåˆ°JSON
            for res in result:
                res.save_to_json(self.output_dir)

            # ä»ç»“æœä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡å­—
            json_file = os.path.join(
                self.output_dir,
                os.path.basename(image_path).replace(".png", "_res.json"),
            )
            return self._find_all_matching_texts_in_json(
                json_file, target_text, confidence_threshold
            )

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…æ–‡å­—æ—¶å‡ºé”™: {e}")
            return []

    def _find_all_matching_texts_in_json(
        self, json_file_path, target_text, confidence_threshold=0.5
    ):
        """
        ä»JSONæ–‡ä»¶ä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡å­—

        Args:
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)

        Returns:
            list: æ‰€æœ‰åŒ¹é…çš„æ–‡å­—ä¿¡æ¯åˆ—è¡¨
        """
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])

            matches = []
            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                # æ£€æŸ¥ç½®ä¿¡åº¦å’Œæ–‡å­—åŒ¹é…ï¼ˆè¯†åˆ«å‡ºçš„æ–‡å­—åŒ…å«ç›®æ ‡æ–‡å­—ï¼‰
                if score >= confidence_threshold and target_text in text:
                    if i < len(dt_polys):
                        poly = dt_polys[i]
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        center_x = int(sum(x_coords) / len(x_coords))
                        center_y = int(sum(y_coords) / len(y_coords))

                        matches.append(
                            {
                                "center": (center_x, center_y),
                                "text": text,
                                "confidence": score,
                                "bbox": poly,
                                "index": len(matches) + 1,
                            }
                        )

            return matches

        except Exception as e:
            self.logger.error(f"å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []

    def get_all_texts_from_image(self, image_path):
        """
        è·å–å›¾åƒä¸­æ‰€æœ‰è¯†åˆ«åˆ°çš„æ–‡å­—ä¿¡æ¯

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„

        Returns:
            list: åŒ…å«æ‰€æœ‰æ–‡å­—ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸åŒ…å«text, confidence, center, bbox
        """
        try:
            # OCR è¯†åˆ«
            result = self._predict_with_timing(image_path)

            if not result or len(result) == 0:
                self.logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return []

            # ä¿å­˜è¯†åˆ«ç»“æœåˆ°JSON
            for res in result:
                res.save_to_json(self.output_dir)

            # ä»JSONæ–‡ä»¶è¯»å–æ‰€æœ‰æ–‡å­—
            json_file = os.path.join(
                self.output_dir,
                os.path.basename(image_path).replace(".png", "_res.json"),
            )
            return self._get_all_texts_from_json(json_file)

        except Exception as e:
            self.logger.error(f"è·å–å›¾åƒæ–‡å­—ä¿¡æ¯å‡ºé”™: {e}")
            return []

    def _get_all_texts_from_json(self, json_file_path):
        """
        ä»JSONæ–‡ä»¶ä¸­è·å–æ‰€æœ‰æ–‡å­—ä¿¡æ¯

        Args:
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„

        Returns:
            list: æ‰€æœ‰æ–‡å­—ä¿¡æ¯åˆ—è¡¨
        """
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])

            texts_info = []

            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                if i < len(dt_polys):
                    poly = dt_polys[i]
                    x_coords = [point[0] for point in poly]
                    y_coords = [point[1] for point in poly]
                    center_x = int(sum(x_coords) / len(x_coords))
                    center_y = int(sum(y_coords) / len(y_coords))

                    texts_info.append(
                        {
                            "text": text,
                            "confidence": score,
                            "center": (center_x, center_y),
                            "bbox": poly,
                        }
                    )

            return texts_info

        except Exception as e:
            self.logger.error(f"è¯»å–JSONæ–‡ä»¶å‡ºé”™: {e}")
            return []
