"""
OCR Helper Class - åŸºäºPaddleOCRçš„æ–‡å­—è¯†åˆ«å’Œå®šä½å·¥å…·ç±»
æä¾›ç»Ÿä¸€çš„æ¥å£ä¾›å¤–éƒ¨è°ƒç”¨ï¼Œè¾“å…¥å›¾åƒæ–‡ä»¶å’Œè¦æŸ¥æ‰¾çš„æ–‡å­—ï¼Œè¿”å›æ–‡å­—æ‰€åœ¨çš„å›¾ç‰‡åŒºåŸŸ
æ”¯æŒåŒºåŸŸåˆ†å‰²åŠŸèƒ½ï¼Œå¯ä»¥æŒ‡å®šåªè¯†åˆ«å±å¹•çš„ç‰¹å®šåŒºåŸŸï¼Œå¤§å¤§æå‡è¯†åˆ«é€Ÿåº¦
"""

import requests
import json
import os
import time
import base64
import cv2
import uuid
import shutil
import sqlite3
import imagehash
from PIL import Image
from datetime import datetime
from airtest.core.api import snapshot, touch
from airtest.aircv.cal_confidence import cal_ccoeff_confidence
from typing import List, Tuple, Optional, Dict, Any, Set
from project_paths import ensure_project_path
from logger_config import setup_logger_from_config, apply_logging_slice


class OCRHelper:
    """OCRè¾…åŠ©å·¥å…·ç±»ï¼Œå°è£…PaddleOCRåŠŸèƒ½"""

    def __init__(
        self,
        output_dir="output",
        use_doc_orientation_classify=False,
        use_doc_unwarping=False,
        use_textline_orientation=False,
        resize_image=True,
        max_width=960,
        delete_temp_screenshots=True,
        max_cache_size=200,
        hash_type="dhash",  # å¯é€‰: "phash", "dhash", "ahash", "whash"
        hash_threshold=10,  # hash æ±‰æ˜è·ç¦»é˜ˆå€¼
        cpu_threads: Optional[int] = None,
    ):
        """
        åˆå§‹åŒ–OCR Helper

        Args:
            output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
            use_doc_orientation_classify (bool): æ˜¯å¦ä½¿ç”¨æ–‡æ¡£æ–¹å‘åˆ†ç±»æ¨¡å‹
            use_doc_unwarping (bool): æ˜¯å¦ä½¿ç”¨æ–‡æœ¬å›¾åƒçŸ«æ­£æ¨¡å‹
            use_textline_orientation (bool): æ˜¯å¦ä½¿ç”¨æ–‡æœ¬è¡Œæ–¹å‘åˆ†ç±»æ¨¡å‹
            resize_image (bool): æ˜¯å¦è‡ªåŠ¨ç¼©å°å›¾ç‰‡ä»¥æå‡é€Ÿåº¦
            max_width (int): å›¾ç‰‡æœ€å¤§å®½åº¦ï¼Œé»˜è®¤960ï¼ˆå»ºè®®åœ¨640-960ä¹‹é—´ï¼‰
            delete_temp_screenshots (bool): æ˜¯å¦åˆ é™¤ä¸´æ—¶æˆªå›¾æ–‡ä»¶ï¼Œé»˜è®¤ä¸ºTrue
            max_cache_size (int): æœ€å¤§ç¼“å­˜æ¡ç›®æ•°ï¼Œé»˜è®¤200
            hash_type (str): å“ˆå¸Œç®—æ³•ç±»å‹ï¼Œé»˜è®¤"dhash"ï¼ˆå·®åˆ†å“ˆå¸Œï¼Œæœ€å¿«ï¼‰
            hash_threshold (int): å“ˆå¸Œæ±‰æ˜è·ç¦»é˜ˆå€¼ï¼Œé»˜è®¤10
        """
        resolved_output_dir = ensure_project_path(output_dir)
        self.output_dir = str(resolved_output_dir)
        self.resize_image = resize_image
        self.max_width = max_width
        self.delete_temp_screenshots = delete_temp_screenshots
        self.max_cache_size = max_cache_size
        self.hash_type = hash_type
        self.hash_threshold = hash_threshold

        self.ocr_url = os.getenv("OCR_SERVER_URL", "http://localhost:8080/ocr")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        # åˆ›å»ºç¼“å­˜ç›®å½•å’Œä¸´æ—¶ç›®å½•
        self.cache_dir = os.path.join(self.output_dir, "cache")
        self.temp_dir = os.path.join(self.output_dir, "temp")
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.temp_dir, exist_ok=True)

        # é…ç½®å½©è‰²æ—¥å¿—ï¼ˆä»ç³»ç»Ÿé…ç½®æ–‡ä»¶åŠ è½½é€šç”¨æ—¥å¿—é…ç½®ï¼‰
        # éœ€è¦å…ˆåˆå§‹åŒ–ï¼Œå› ä¸ºç¼“å­˜åŠ è½½æ—¶ä¼šç”¨åˆ°
        self.logger = setup_logger_from_config(use_color=True)

        # åˆå§‹åŒ–ç¼“å­˜
        # æ ¼å¼: [(image_path, json_file_path), ...]
        self.ocr_cache = []

        # ç¼“å­˜ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ95%ä»¥ä¸Šè®¤ä¸ºæ˜¯åŒä¸€å¼ å›¾ï¼‰
        self.cache_similarity_threshold = 0.95

        # åˆå§‹åŒ– SQLite ç¼“å­˜æ•°æ®åº“
        self.cache_db_path = os.path.join(self.cache_dir, "cache.db")
        self._init_cache_db()

        # åŠ è½½å·²æœ‰çš„ç¼“å­˜ï¼ˆéœ€è¦åœ¨ logger åˆå§‹åŒ–ä¹‹åï¼‰
        self._load_existing_cache()

    def _init_cache_db(self):
        """
        åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“ï¼Œåˆ›å»ºå¿…è¦çš„è¡¨
        """
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.cursor()
                # åˆ›å»ºç¼“å­˜è¡¨
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS cache_entries (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        image_path TEXT UNIQUE NOT NULL,
                        json_path TEXT NOT NULL,
                        phash TEXT,
                        dhash TEXT,
                        ahash TEXT,
                        whash TEXT,
                        regions TEXT,  -- JSON å­˜å‚¨åŒºåŸŸä¿¡æ¯
                        hit_count INTEGER DEFAULT 0,
                        last_access_time REAL,
                        created_time REAL,
                        image_size INTEGER,  -- å›¾ç‰‡æ–‡ä»¶å¤§å°
                        image_hash TEXT  -- MD5 hash for exact duplicate detection
                    )
                """)
                # åˆ›å»ºç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_phash ON cache_entries(phash)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_dhash ON cache_entries(dhash)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_last_access ON cache_entries(last_access_time)"
                )
                cursor.execute(
                    "CREATE INDEX IF NOT EXISTS idx_image_hash ON cache_entries(image_hash)"
                )
                conn.commit()
            self.logger.debug(f"âœ… ç¼“å­˜æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ: {self.cache_db_path}")
        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“å¤±è´¥: {e}")
            raise

    def _compute_image_hash(
        self, image_path: str, hash_type: Optional[str] = None
    ) -> Optional[str]:
        """
        è®¡ç®—å›¾åƒçš„æ„ŸçŸ¥å“ˆå¸Œå€¼

        Args:
            image_path: å›¾åƒè·¯å¾„
            hash_type: å“ˆå¸Œç±»å‹ ("phash", "dhash", "ahash", "whash")

        Returns:
            å“ˆå¸Œå€¼çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å›None
        """
        if hash_type is None:
            hash_type = self.hash_type

        try:
            with Image.open(image_path) as img:
                if hash_type == "phash":
                    hash_obj = imagehash.phash(img)
                elif hash_type == "dhash":
                    hash_obj = imagehash.dhash(img)
                elif hash_type == "ahash":
                    hash_obj = imagehash.average_hash(img)
                elif hash_type == "whash":
                    hash_obj = imagehash.whash(img)
                else:
                    self.logger.warning(f"æœªçŸ¥çš„å“ˆå¸Œç±»å‹: {hash_type}ï¼Œä½¿ç”¨é»˜è®¤ dhash")
                    hash_obj = imagehash.dhash(img)
                return str(hash_obj)
        except Exception as e:
            self.logger.debug(f"è®¡ç®—å›¾åƒå“ˆå¸Œå¤±è´¥ ({image_path}): {e}")
            return None

    def _compute_all_hashes(self, image_path: str) -> Dict[str, str]:
        """
        è®¡ç®—å›¾åƒçš„æ‰€æœ‰å“ˆå¸Œå€¼

        Args:
            image_path: å›¾åƒè·¯å¾„

        Returns:
            åŒ…å«æ‰€æœ‰å“ˆå¸Œå€¼çš„å­—å…¸
        """
        hashes = {}
        try:
            with Image.open(image_path) as img:
                hashes["phash"] = str(imagehash.phash(img))
                hashes["dhash"] = str(imagehash.dhash(img))
                hashes["ahash"] = str(imagehash.average_hash(img))
                hashes["whash"] = str(imagehash.whash(img))
        except Exception as e:
            self.logger.debug(f"è®¡ç®—å›¾åƒå“ˆå¸Œå¤±è´¥ ({image_path}): {e}")
        return hashes

    def _calculate_hamming_distance(self, hash1: str, hash2: str) -> int:
        """
        è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œå€¼ä¹‹é—´çš„æ±‰æ˜è·ç¦»

        Args:
            hash1: ç¬¬ä¸€ä¸ªå“ˆå¸Œå€¼
            hash2: ç¬¬äºŒä¸ªå“ˆå¸Œå€¼

        Returns:
            æ±‰æ˜è·ç¦»ï¼ˆä¸åŒä½çš„æ•°é‡ï¼‰
        """
        try:
            # å°†åå…­è¿›åˆ¶è½¬æ¢ä¸ºäºŒè¿›åˆ¶å­—ç¬¦ä¸²
            h1 = int(hash1, 16)
            h2 = int(hash2, 16)
            # å¼‚æˆ–åè®¡ç®—1çš„ä¸ªæ•°
            return bin(h1 ^ h2).count("1")
        except Exception:
            return 999  # è¿”å›ä¸€ä¸ªå¤§å€¼è¡¨ç¤ºæ— æ³•æ¯”è¾ƒ

    def _get_cache_key(
        self, image_path: str, regions: Optional[List[int]] = None
    ) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŒ…å«å›¾åƒè·¯å¾„å’ŒåŒºåŸŸä¿¡æ¯

        Args:
            image_path: å›¾åƒè·¯å¾„
            regions: åŒºåŸŸåˆ—è¡¨

        Returns:
            å”¯ä¸€çš„ç¼“å­˜é”®
        """
        if regions:
            regions_str = "_".join(map(str, sorted(regions)))
            return f"{image_path}_{regions_str}"
        return image_path

    def _find_similar_in_cache(
        self, image_path: str, regions: Optional[List[int]] = None
    ) -> Optional[str]:
        """
        åœ¨ç¼“å­˜ä¸­æŸ¥æ‰¾ç›¸ä¼¼çš„å›¾åƒ

        Args:
            image_path: è¦æŸ¥æ‰¾çš„å›¾åƒè·¯å¾„
            regions: åŒºåŸŸåˆ—è¡¨ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰

        Returns:
            æ‰¾åˆ°çš„JSONæ–‡ä»¶è·¯å¾„ï¼Œæ²¡æ‰¾åˆ°è¿”å›None
        """
        try:
            # è®¡ç®—å½“å‰å›¾åƒçš„å“ˆå¸Œå€¼
            current_hashes = self._compute_all_hashes(image_path)
            if not current_hashes.get(self.hash_type):
                return None

            # è¿æ¥æ•°æ®åº“
            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.cursor()

                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å®Œå…¨ç›¸åŒçš„å›¾åƒï¼ˆé€šè¿‡æ–‡ä»¶å“ˆå¸Œï¼‰
                import hashlib

                with open(image_path, "rb") as f:
                    file_hash = hashlib.md5(f.read()).hexdigest()

                cursor.execute(
                    "SELECT json_path FROM cache_entries WHERE image_hash = ?",
                    (file_hash,),
                )
                result = cursor.fetchone()
                if result:
                    self.logger.debug(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­ï¼ˆå®Œå…¨ç›¸åŒï¼‰: {result[0]}")
                    # æ›´æ–°è®¿é—®ä¿¡æ¯
                    cursor.execute(
                        "UPDATE cache_entries SET hit_count = hit_count + 1, last_access_time = ? WHERE image_hash = ?",
                        (time.time(), file_hash),
                    )
                    conn.commit()
                    return result[0]

                # æŸ¥æ‰¾ç›¸ä¼¼çš„å›¾åƒï¼ˆåŸºäºæ„ŸçŸ¥å“ˆå¸Œï¼‰
                # ä½¿ç”¨ä¸»è¦å“ˆå¸Œç±»å‹è¿›è¡ŒæŸ¥æ‰¾
                primary_hash = current_hashes[self.hash_type]

                # æ„å»ºæŸ¥è¯¢ - æŸ¥æ‰¾æ±‰æ˜è·ç¦»å°äºé˜ˆå€¼çš„æ¡ç›®
                # æ³¨æ„ï¼šSQLiteæ²¡æœ‰å†…ç½®çš„æ±‰æ˜è·ç¦»å‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬è·å–æ‰€æœ‰è®°å½•å¹¶åœ¨Pythonä¸­è®¡ç®—
                cursor.execute(f"""
                    SELECT image_path, json_path, {self.hash_type}, hit_count, last_access_time
                    FROM cache_entries
                    WHERE {self.hash_type} IS NOT NULL
                    ORDER BY last_access_time DESC
                    LIMIT 100
                """)

                candidates = cursor.fetchall()
                best_match = None
                best_distance = 999

                for (
                    img_path,
                    json_path,
                    cached_hash,
                    hit_count,
                    last_access,
                ) in candidates:
                    if not cached_hash:
                        continue

                    distance = self._calculate_hamming_distance(
                        primary_hash, cached_hash
                    )
                    if distance < best_distance and distance <= self.hash_threshold:
                        best_distance = distance
                        best_match = (json_path, distance, hit_count)

                if best_match:
                    json_path, distance, hit_count = best_match
                    self.logger.debug(
                        f"ğŸ’¾ ç¼“å­˜å‘½ä¸­ï¼ˆå“ˆå¸Œç›¸ä¼¼ï¼Œè·ç¦»={distance}ï¼‰: {json_path}"
                    )

                    # æ›´æ–°è®¿é—®ä¿¡æ¯
                    cursor.execute(
                        "UPDATE cache_entries SET hit_count = hit_count + 1, last_access_time = ? WHERE json_path = ?",
                        (time.time(), json_path),
                    )
                    conn.commit()

                    return json_path

            return None
        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾ç¼“å­˜å¤±è´¥: {e}")
            return None

    def _evict_cache(self):
        """
        æ·˜æ±°æœ€ä¹…æœªè®¿é—®çš„ç¼“å­˜æ¡ç›®ï¼Œä¿æŒç¼“å­˜å¤§å°åœ¨é™åˆ¶å†…
        """
        try:
            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.cursor()

                # è·å–å½“å‰ç¼“å­˜æ¡ç›®æ•°
                cursor.execute("SELECT COUNT(*) FROM cache_entries")
                count = cursor.fetchone()[0]

                if count > self.max_cache_size:
                    # è®¡ç®—éœ€è¦åˆ é™¤çš„æ¡ç›®æ•°
                    to_delete = (
                        count - self.max_cache_size + 10
                    )  # å¤šåˆ é™¤ä¸€äº›ï¼Œé¿å…é¢‘ç¹æ“ä½œ

                    # è·å–æœ€ä¹…æœªè®¿é—®çš„æ¡ç›®
                    cursor.execute(
                        """
                        SELECT image_path, json_path
                        FROM cache_entries
                        ORDER BY last_access_time ASC
                        LIMIT ?
                    """,
                        (to_delete,),
                    )

                    to_evict = cursor.fetchall()

                    # åˆ é™¤æ–‡ä»¶å’Œæ•°æ®åº“è®°å½•
                    for img_path, json_path in to_evict:
                        try:
                            if os.path.exists(img_path):
                                os.remove(img_path)
                            if os.path.exists(json_path):
                                os.remove(json_path)
                        except Exception:
                            pass

                    # ä»æ•°æ®åº“åˆ é™¤è®°å½•
                    cursor.execute(
                        """
                        DELETE FROM cache_entries
                        WHERE id IN (
                            SELECT id FROM cache_entries
                            ORDER BY last_access_time ASC
                            LIMIT ?
                        )
                    """,
                        (to_delete,),
                    )

                    conn.commit()
                    self.logger.debug(f"ğŸ—‘ï¸ æ·˜æ±°äº† {to_delete} ä¸ªç¼“å­˜æ¡ç›®")
        except Exception as e:
            self.logger.error(f"æ·˜æ±°ç¼“å­˜å¤±è´¥: {e}")

    def _save_to_cache_db(
        self, image_path: str, json_path: str, regions: Optional[List[int]] = None
    ):
        """
        ä¿å­˜ç¼“å­˜æ¡ç›®åˆ°æ•°æ®åº“

        Args:
            image_path: å›¾åƒè·¯å¾„
            json_path: JSONæ–‡ä»¶è·¯å¾„
            regions: åŒºåŸŸåˆ—è¡¨
        """
        try:
            # è®¡ç®—æ‰€æœ‰å“ˆå¸Œå€¼
            hashes = self._compute_all_hashes(image_path)

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            import hashlib

            with open(image_path, "rb") as f:
                file_hash = hashlib.md5(f.read()).hexdigest()

            # è·å–æ–‡ä»¶å¤§å°
            image_size = os.path.getsize(image_path)

            # å‡†å¤‡åŒºåŸŸä¿¡æ¯
            regions_json = json.dumps(sorted(regions)) if regions else None

            with sqlite3.connect(self.cache_db_path) as conn:
                cursor = conn.cursor()

                # æ’å…¥æˆ–æ›´æ–°è®°å½•
                cursor.execute(
                    """
                    INSERT OR REPLACE INTO cache_entries
                    (image_path, json_path, phash, dhash, ahash, whash, regions,
                     hit_count, last_access_time, created_time, image_size, image_hash)
                    VALUES (?, ?, ?, ?, ?, ?, ?, 1, ?, ?, ?, ?)
                """,
                    (
                        image_path,
                        json_path,
                        hashes.get("phash"),
                        hashes.get("dhash"),
                        hashes.get("ahash"),
                        hashes.get("whash"),
                        regions_json,
                        time.time(),
                        time.time(),
                        image_size,
                        file_hash,
                    ),
                )

                conn.commit()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·˜æ±°
            self._evict_cache()

        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")

    def _merge_regions(self, regions: List[int]) -> Tuple[int, int, int, int]:
        """
        åˆå¹¶å¤šä¸ªåŒºåŸŸä¸ºä¸€ä¸ªè¿ç»­çš„çŸ©å½¢åŒºåŸŸ

        Args:
            regions: è¦åˆå¹¶çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰
                    1 2 3
                    4 5 6
                    7 8 9

        Returns:
            åˆå¹¶åçš„è¾¹ç•Œ (min_row, max_row, min_col, max_col)ï¼Œéƒ½æ˜¯0-basedç´¢å¼•
        """
        if not regions:
            return (0, 2, 0, 2)  # æ•´ä¸ªå›¾åƒ

        # å°†åŒºåŸŸIDè½¬æ¢ä¸ºè¡Œåˆ—ç´¢å¼•
        rows = []
        cols = []
        for region_id in regions:
            if not 1 <= region_id <= 9:
                self.logger.warning(f"æ— æ•ˆçš„åŒºåŸŸID: {region_id}ï¼Œè·³è¿‡")
                continue
            row = (region_id - 1) // 3
            col = (region_id - 1) % 3
            rows.append(row)
            cols.append(col)

        if not rows:
            return (0, 2, 0, 2)

        # è®¡ç®—åŒ…å«æ‰€æœ‰åŒºåŸŸçš„æœ€å°çŸ©å½¢
        min_row = min(rows)
        max_row = max(rows)
        min_col = min(cols)
        max_col = max(cols)

        return (min_row, max_row, min_col, max_col)

    def _get_region_bounds(
        self, image_shape: Tuple[int, int], regions: Optional[List[int]] = None
    ) -> Tuple[int, int, int, int]:
        """
        å°†å›¾åƒåˆ†æˆ3x3ç½‘æ ¼ï¼Œè¿”å›åˆå¹¶åçš„åŒºåŸŸè¾¹ç•Œ

        Args:
            image_shape: å›¾åƒå½¢çŠ¶ (height, width)
            regions: è¦æå–çš„åŒºåŸŸåˆ—è¡¨ï¼Œä½¿ç”¨æ•°å­—1-9è¡¨ç¤ºï¼ˆä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹ï¼‰
                    1 2 3
                    4 5 6
                    7 8 9
                    å¦‚æœä¸ºNoneï¼Œè¿”å›æ•´ä¸ªå›¾åƒ
                    å¤šä¸ªåŒºåŸŸä¼šè¢«åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„çŸ©å½¢

        Returns:
            åŒºåŸŸè¾¹ç•Œ (x, y, w, h)
        """
        height, width = image_shape

        if regions is None:
            # è¿”å›æ•´ä¸ªå›¾åƒ
            return (0, 0, width, height)

        # åˆå¹¶åŒºåŸŸ
        min_row, max_row, min_col, max_col = self._merge_regions(regions)

        # è®¡ç®—æ¯ä¸ªæ ¼å­çš„å¤§å°
        cell_height = height // 3
        cell_width = width // 3

        # è®¡ç®—åˆå¹¶åçš„è¾¹ç•Œ
        x = min_col * cell_width
        y = min_row * cell_height
        w = (max_col - min_col + 1) * cell_width
        h = (max_row - min_row + 1) * cell_height

        # å¤„ç†è¾¹ç•Œæƒ…å†µï¼Œç¡®ä¿è¦†ç›–åˆ°å›¾åƒè¾¹ç¼˜
        if max_col == 2:  # åŒ…å«æœ€å³åˆ—
            w = width - x
        if max_row == 2:  # åŒ…å«æœ€ä¸‹è¡Œ
            h = height - y

        return (x, y, w, h)

    def _extract_region(
        self,
        image: Any,
        regions: Optional[List[int]] = None,
        debug_save_path: Optional[str] = None,
    ) -> Tuple[Any, Tuple[int, int]]:
        """
        ä»å›¾åƒä¸­æå–æŒ‡å®šçš„åŒºåŸŸï¼ˆåˆå¹¶åçš„å•ä¸ªåŒºåŸŸï¼‰

        Args:
            image: OpenCVå›¾åƒå¯¹è±¡
            regions: è¦æå–çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œä¼šè¢«åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„çŸ©å½¢
            debug_save_path: è°ƒè¯•ç”¨ï¼Œä¿å­˜åŒºåŸŸæˆªå›¾çš„è·¯å¾„

        Returns:
            (region_image, (offset_x, offset_y))
        """
        if image is None:
            return None, (0, 0)

        height, width = image.shape[:2]
        x, y, w, h = self._get_region_bounds((height, width), regions)

        region_img = image[y : y + h, x : x + w]

        # è°ƒè¯•ï¼šä¿å­˜åŒºåŸŸæˆªå›¾
        if debug_save_path:
            import cv2

            cv2.imwrite(debug_save_path, region_img)
            self.logger.debug(f"ğŸ” è°ƒè¯•ï¼šåŒºåŸŸæˆªå›¾å·²ä¿å­˜åˆ° {debug_save_path}")
            self.logger.debug(f"   åŒºåŸŸèŒƒå›´: x={x}, y={y}, w={w}, h={h}")
            self.logger.debug(f"   åŸå›¾å°ºå¯¸: {width}x{height}")

        return region_img, (x, y)

    def _adjust_coordinates_to_full_image(
        self, bbox: List[List[int]], offset: Tuple[int, int]
    ) -> List[List[int]]:
        """
        å°†åŒºåŸŸå†…çš„åæ ‡è°ƒæ•´ä¸ºåŸå›¾ä¸­çš„åæ ‡

        Args:
            bbox: åŒºåŸŸå†…çš„è¾¹ç•Œæ¡†åæ ‡ [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
            offset: åŒºåŸŸåœ¨åŸå›¾ä¸­çš„åç§»é‡ (offset_x, offset_y)

        Returns:
            è°ƒæ•´åçš„è¾¹ç•Œæ¡†åæ ‡
        """
        offset_x, offset_y = offset
        adjusted_bbox = []
        for point in bbox:
            adjusted_bbox.append([point[0] + offset_x, point[1] + offset_y])
        return adjusted_bbox

    def _load_existing_cache(self):
        """
        åŠ è½½ç¼“å­˜ç›®å½•ä¸­å·²æœ‰çš„ç¼“å­˜æ–‡ä»¶
        """
        try:
            if not os.path.exists(self.cache_dir):
                return

            # æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜æ–‡ä»¶å¯¹
            cache_files = os.listdir(self.cache_dir)
            cache_pairs = {}

            # å°†å›¾ç‰‡å’Œ JSON æ–‡ä»¶é…å¯¹
            for filename in cache_files:
                if filename.startswith("cache_") and filename.endswith(".png"):
                    # æå–ç¼“å­˜ ID
                    cache_id = filename.replace("cache_", "").replace(".png", "")
                    json_filename = f"cache_{cache_id}_res.json"

                    image_path = os.path.join(self.cache_dir, filename)
                    json_path = os.path.join(self.cache_dir, json_filename)

                    # æ£€æŸ¥å¯¹åº”çš„ JSON æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                    if os.path.exists(json_path):
                        cache_pairs[cache_id] = (image_path, json_path)

            # æŒ‰ ID æ’åºå¹¶åŠ è½½åˆ°ç¼“å­˜åˆ—è¡¨
            for cache_id in sorted(
                cache_pairs.keys(), key=lambda x: int(x) if x.isdigit() else 0
            ):
                self.ocr_cache.append(cache_pairs[cache_id])

            if self.ocr_cache:
                self.logger.debug(f"ğŸ’¾ åŠ è½½äº† {len(self.ocr_cache)} ä¸ªç¼“å­˜æ–‡ä»¶")
        except Exception as e:
            self.logger.error(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")

    def _find_similar_cached_image(
        self, current_image_path, regions: Optional[List[int]] = None
    ):
        """
        æŸ¥æ‰¾ç¼“å­˜ä¸­æ˜¯å¦æœ‰ç›¸ä¼¼çš„å›¾ç‰‡ï¼ˆä½¿ç”¨æ–°çš„å“ˆå¸Œç´¢å¼•ç³»ç»Ÿï¼‰

        Args:
            current_image_path (str): å½“å‰å›¾ç‰‡è·¯å¾„
            regions (List[int], optional): åŒºåŸŸåˆ—è¡¨

        Returns:
            str: ç¼“å­˜çš„ JSON æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        # é¦–å…ˆå°è¯•ä½¿ç”¨æ–°çš„å“ˆå¸Œç´¢å¼•ç³»ç»Ÿ
        cached_json = self._find_similar_in_cache(current_image_path, regions)
        if cached_json:
            return cached_json

        # å¦‚æœæ–°ç³»ç»Ÿæ²¡æ‰¾åˆ°ï¼Œå›é€€åˆ°æ—§ç³»ç»Ÿï¼ˆå…¼å®¹æ€§ï¼‰
        try:
            current_img = cv2.imread(current_image_path)
            if current_img is None:
                return None

            # éå†ç¼“å­˜ï¼ŒæŸ¥æ‰¾ç›¸ä¼¼å›¾ç‰‡
            for cached_img_path, cached_json_path in self.ocr_cache:
                if not os.path.exists(cached_img_path) or not os.path.exists(
                    cached_json_path
                ):
                    continue

                cached_img = cv2.imread(cached_img_path)
                if cached_img is None:
                    continue

                # è°ƒæ•´å›¾ç‰‡å°ºå¯¸ä¸€è‡´ä»¥ä¾¿æ¯”è¾ƒ
                if current_img.shape != cached_img.shape:
                    cached_img = cv2.resize(
                        cached_img, (current_img.shape[1], current_img.shape[0])
                    )

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = cal_ccoeff_confidence(current_img, cached_img)

                if similarity >= self.cache_similarity_threshold:
                    self.logger.debug(
                        f"ğŸ’¾ æ‰¾åˆ°ç›¸ä¼¼ç¼“å­˜å›¾ç‰‡ï¼ˆæ—§ç³»ç»Ÿï¼‰(ç›¸ä¼¼åº¦: {similarity * 100:.1f}%)"
                    )
                    return cached_json_path

            return None
        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾ç›¸ä¼¼ç¼“å­˜å›¾ç‰‡å¤±è´¥: {e}")
            return None

    def _save_to_cache(
        self, image_path, json_file, regions: Optional[List[int]] = None
    ):
        """
        ä¿å­˜å›¾ç‰‡å’Œ OCR ç»“æœåˆ°ç¼“å­˜ï¼ˆä½¿ç”¨æ–°çš„ SQLite ç³»ç»Ÿï¼‰

        Args:
            image_path (str): å›¾ç‰‡è·¯å¾„
            json_file (str): JSON æ–‡ä»¶è·¯å¾„
            regions (List[int], optional): åŒºåŸŸåˆ—è¡¨
        """
        try:
            # ä¸ºç¼“å­˜åˆ›å»ºå”¯ä¸€çš„æ–‡ä»¶å
            cache_id = len(self.ocr_cache)
            cache_image_name = f"cache_{cache_id}.png"
            cache_json_name = f"cache_{cache_id}_res.json"

            cache_image_path = os.path.join(self.cache_dir, cache_image_name)
            cache_json_path = os.path.join(self.cache_dir, cache_json_name)

            # å¤åˆ¶å›¾ç‰‡åˆ°ç¼“å­˜ç›®å½•
            shutil.copy2(image_path, cache_image_path)

            # å¤åˆ¶ JSON åˆ°ç¼“å­˜ç›®å½•
            if os.path.exists(json_file):
                shutil.copy2(json_file, cache_json_path)
                # ä¿å­˜ç¼“å­˜è®°å½•åˆ°æ—§ç³»ç»Ÿï¼ˆå…¼å®¹æ€§ï¼‰
                self.ocr_cache.append((cache_image_path, cache_json_path))
                self.logger.debug(
                    f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜ (å›¾ç‰‡æ•°: {len(self.ocr_cache)}, JSON: {cache_json_name})"
                )

                # ä¿å­˜åˆ°æ–°çš„ SQLite ç³»ç»Ÿ
                self._save_to_cache_db(cache_image_path, cache_json_path, regions)
            else:
                self.logger.error(f"JSON æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ç¼“å­˜: {json_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

    def _resize_image_for_ocr(self, image_path):
        """
        è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥åŠ é€Ÿ OCR è¯†åˆ«

        Args:
            image_path (str): åŸå§‹å›¾ç‰‡è·¯å¾„

        Returns:
            str: è°ƒæ•´åçš„å›¾ç‰‡è·¯å¾„ï¼ˆå¦‚æœéœ€è¦è°ƒæ•´ï¼‰ï¼Œå¦åˆ™è¿”å›åŸè·¯å¾„
        """
        if not self.resize_image:
            return image_path

        try:
            img = cv2.imread(image_path)
            if img is None:
                return image_path

            height, width = img.shape[:2]

            # å¦‚æœå›¾ç‰‡å®½åº¦å¤§äºæœ€å¤§å®½åº¦ï¼Œè¿›è¡Œç¼©æ”¾
            if width > self.max_width:
                scale = self.max_width / width
                new_width = self.max_width
                new_height = int(height * scale)

                # ç¼©å°å›¾ç‰‡
                resized_img = cv2.resize(
                    img, (new_width, new_height), interpolation=cv2.INTER_AREA
                )

                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_path = image_path.replace(".png", "_resized.png")
                cv2.imwrite(temp_path, resized_img)

                self.logger.debug(
                    f"ğŸ”§ å›¾ç‰‡å·²ç¼©å°: {width}x{height} -> {new_width}x{new_height}"
                )
                return temp_path

            return image_path
        except Exception as e:
            self.logger.warning(f"å›¾ç‰‡ç¼©æ”¾å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå›¾")
            return image_path

    def _predict_with_timing(self, image_path):
        """
        æ‰§è¡Œ OCR è¯†åˆ«å¹¶è®°å½•è€—æ—¶ (Remote PaddleX 3.0)

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„

        Returns:
            OCR è¯†åˆ«ç»“æœ (dict: {rec_texts: [], rec_scores: [], dt_polys: []})
        """
        # é¢„å¤„ç†ï¼šç¼©å°å›¾ç‰‡
        processed_image_path = self._resize_image_for_ocr(image_path)

        start_time = time.time()
        result = None

        try:
            # 1. è½¬æ¢ä¸º Base64
            with open(processed_image_path, "rb") as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')

            # 2. æ„å»ºç¬¦åˆ PaddleX 3.0 è§„èŒƒçš„ Payload
            payload = {
                "file": image_data,
                "fileType": 1
            }
            
            # 3. å‘é€è¯·æ±‚ (é»˜è®¤ç«¯å£ 8080)
            response = requests.post(self.ocr_url, json=payload, timeout=30)
            response.raise_for_status()
            json_resp = response.json()

            if json_resp.get("errorCode") == 0:
                # PaddleX 3.0 çš„ç»“æœåµŒå¥—åœ¨ result.ocrResults[0].prunedResult ä¸­
                ocr_results = json_resp.get("result", {}).get("ocrResults", [])
                if ocr_results:
                    pruned = ocr_results[0].get("prunedResult", {})
                    
                    # è½¬æ¢æ ¼å¼ä¸º OCRHelper æ‰€éœ€çš„æ ¼å¼
                    result = {
                        "rec_texts": pruned.get("rec_texts", []),
                        "rec_scores": pruned.get("rec_scores", []),
                        "dt_polys": pruned.get("dt_polys", [])
                    }
                else:
                    self.logger.warning(f"OCR Server returned empty ocrResults")
            else:
                self.logger.error(f"OCR Server Error: {json_resp.get('errorMsg')}")

        except Exception as e:
            self.logger.error(f"OCR Request Failed: {e}")

        elapsed_time = time.time() - start_time

        filename = os.path.basename(image_path)
        self.logger.debug(f"â±ï¸ OCRè¯†åˆ«è€—æ—¶: {elapsed_time:.3f}ç§’ (æ–‡ä»¶: {filename})")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if processed_image_path != image_path and os.path.exists(processed_image_path):
            try:
                os.remove(processed_image_path)
            except Exception:
                pass

        return result

    def _get_or_create_ocr_result(
        self, image_path, use_cache=True, regions: Optional[List[int]] = None
    ):
        """
        è·å–æˆ–åˆ›å»º OCR è¯†åˆ«ç»“æœï¼ˆå¸¦ç¼“å­˜ï¼‰

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): åŒºåŸŸåˆ—è¡¨

        Returns:
            str: JSON æ–‡ä»¶è·¯å¾„
        """
        # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œæ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰ç›¸ä¼¼å›¾ç‰‡
        if use_cache:
            cached_json = self._find_similar_cached_image(image_path, regions)
            if cached_json:
                return cached_json

        # ç¼“å­˜æœªå‘½ä¸­æˆ–ç¦ç”¨ç¼“å­˜ï¼Œæ‰§è¡Œ OCR è¯†åˆ«
        result = self._predict_with_timing(image_path)

        if result:
            # ä½¿ç”¨å®Œæ•´çš„æ–‡ä»¶è·¯å¾„ä¿å­˜åˆ° cache ç›®å½•
            json_filename = os.path.basename(image_path).replace(
                ".png", "_res.json"
            )
            json_path = os.path.join(self.cache_dir, json_filename)

            # ä¿å­˜ JSON
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False)

            # è·å– JSON æ–‡ä»¶è·¯å¾„
            json_file = json_path

            # å¦‚æœå¯ç”¨ç¼“å­˜ï¼ŒåŒæ—¶ä¿å­˜åˆ°ç¼“å­˜
            if use_cache and os.path.exists(json_file):
                self._save_to_cache(image_path, json_file, regions)

            return json_file

        return None

    def find_text_in_image(
        self,
        image_path,
        target_text,
        confidence_threshold=0.5,
        occurrence=1,
        use_cache=True,
        regions: Optional[List[int]] = None,
        debug_save_path: Optional[str] = None,
    ):
        """
        åœ¨æŒ‡å®šå›¾åƒä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—çš„ä½ç½®

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            occurrence (int): æŒ‡å®šç‚¹å‡»ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æœç´¢æ•´ä¸ªå›¾åƒ
                                          åŒºåŸŸç¼–å·å¦‚ä¸‹ï¼š
                                          1 2 3
                                          4 5 6
                                          7 8 9
            debug_save_path (str, optional): è°ƒè¯•ç”¨ï¼Œä¿å­˜åŒºåŸŸæˆªå›¾çš„è·¯å¾„

        Returns:
            dict: åŒ…å«ä»¥ä¸‹ä¿¡æ¯çš„å­—å…¸
                - found (bool): æ˜¯å¦æ‰¾åˆ°æ–‡å­—
                - center (tuple): æ–‡å­—ä¸­å¿ƒåæ ‡ (x, y)ï¼Œæœªæ‰¾åˆ°æ—¶ä¸ºNone
                - text (str): å®é™…è¯†åˆ«åˆ°çš„æ–‡å­—ï¼Œæœªæ‰¾åˆ°æ—¶ä¸ºNone
                - confidence (float): ç½®ä¿¡åº¦ï¼Œæœªæ‰¾åˆ°æ—¶ä¸ºNone
                - bbox (list): æ–‡å­—è¾¹ç•Œæ¡†åæ ‡ï¼Œæœªæ‰¾åˆ°æ—¶ä¸ºNone
                - total_matches (int): æ€»å…±æ‰¾åˆ°çš„åŒ¹é…æ•°é‡
                - selected_index (int): å®é™…é€‰æ‹©çš„ç´¢å¼• (1-based)
        """
        try:
            # å¦‚æœæŒ‡å®šäº†åŒºåŸŸï¼Œä½¿ç”¨åŒºåŸŸæœç´¢
            if regions is not None:
                return self._find_text_in_regions(
                    image_path,
                    target_text,
                    confidence_threshold,
                    occurrence,
                    regions,
                    debug_save_path,
                    use_cache,
                )

            # è·å–æˆ–åˆ›å»º OCR ç»“æœï¼ˆå¸¦ç¼“å­˜ï¼‰
            json_file = self._get_or_create_ocr_result(
                image_path, use_cache=use_cache, regions=regions
            )

            if not json_file:
                self.logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return {
                    "found": False,
                    "center": None,
                    "text": None,
                    "confidence": None,
                    "bbox": None,
                    "total_matches": 0,
                    "selected_index": 0,
                }

            # ä» JSON ä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—
            return self._find_text_in_json(
                json_file, target_text, confidence_threshold, occurrence
            )

        except Exception as e:
            self.logger.error(f"å›¾åƒOCRè¯†åˆ«å‡ºé”™: {e}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }

    def _find_text_in_regions(
        self,
        image_path: str,
        target_text: str,
        confidence_threshold: float,
        occurrence: int,
        regions: List[int],
        debug_save_path: Optional[str] = None,
        use_cache: bool = True,
    ) -> Dict[str, Any]:
        """
        åœ¨æŒ‡å®šåŒºåŸŸä¸­æŸ¥æ‰¾æ–‡å­—ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰

        Args:
            image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
            target_text: è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            occurrence: æŒ‡å®šç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­—
            regions: è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆä¼šè¢«åˆå¹¶æˆä¸€ä¸ªè¿ç»­çš„çŸ©å½¢ï¼‰
            debug_save_path: è°ƒè¯•ç”¨ï¼Œä¿å­˜åŒºåŸŸæˆªå›¾çš„è·¯å¾„
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True

        Returns:
            æŸ¥æ‰¾ç»“æœå­—å…¸
        """
        try:
            # è¯»å–å›¾åƒ
            image = cv2.imread(image_path)
            if image is None:
                self.logger.error(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
                return self._empty_result()

            # æå–åˆå¹¶åçš„åŒºåŸŸ
            region_img, offset = self._extract_region(image, regions, debug_save_path)
            if region_img is None:
                self.logger.warning("æœªèƒ½æå–åŒºåŸŸ")
                return self._empty_result()

            # æ˜¾ç¤ºåŒºåŸŸä¿¡æ¯
            region_desc = self._get_region_description(regions)
            self.logger.debug(f"ğŸ” åœ¨{region_desc}æœç´¢æ–‡å­—: '{target_text}'")

            # ä¸ºåŒºåŸŸå›¾åƒç”Ÿæˆç¼“å­˜é”®
            region_key = self._get_cache_key(image_path, regions)
            region_cache_path = os.path.join(
                self.cache_dir, f"region_{hash(region_key) % 1000000}.png"
            )

            # åˆå§‹åŒ–ç»“æœ
            result = None
            cache_used = False
            elapsed_time = 0

            # åªæœ‰åœ¨ä½¿ç”¨ç¼“å­˜æ—¶æ‰å°è¯•ä»ç¼“å­˜è¯»å–
            if use_cache:
                # ä¿å­˜åŒºåŸŸå›¾åƒä»¥ä¾›ç¼“å­˜
                cv2.imwrite(region_cache_path, region_img)

                # å°è¯•ä»ç¼“å­˜è·å–OCRç»“æœ
                cached_json = self._find_similar_in_cache(region_cache_path, regions)
                if cached_json and os.path.exists(cached_json):
                    self.logger.debug(f"ğŸ’¾ åŒºåŸŸç¼“å­˜å‘½ä¸­: {region_desc}")
                    # ä»ç¼“å­˜çš„JSONè¯»å–ç»“æœ
                    with open(cached_json, "r", encoding="utf-8") as f:
                        ocr_data = json.load(f)
                    # è½¬æ¢æ ¼å¼ä»¥å…¼å®¹ç°æœ‰ä»£ç 
                    result = [ocr_data] if isinstance(ocr_data, dict) else ocr_data
                    cache_used = True

            # å¦‚æœæ²¡æœ‰å‘½ä¸­ç¼“å­˜æˆ–ä¸ä½¿ç”¨ç¼“å­˜ï¼Œè¿›è¡ŒOCRè¯†åˆ«
            if result is None:
                # å¯¹åŒºåŸŸè¿›è¡ŒOCRè¯†åˆ«
                start_time = time.time()
                result = self.ocr.predict(region_img)
                elapsed_time = time.time() - start_time
                self.logger.debug(
                    f"â±ï¸ åŒºåŸŸ OCR è€—æ—¶: {elapsed_time:.3f}ç§’ (åç§»: {offset})"
                )

                # ä¿å­˜OCRç»“æœåˆ°ç¼“å­˜ï¼ˆä»…åœ¨ä½¿ç”¨ç¼“å­˜æ—¶ï¼‰
                if use_cache and result and len(result) > 0:
                    # ä¿å­˜OCRç»“æœ
                    region_json_path = region_cache_path.replace(".png", "_res.json")
                    # ç›´æ¥ä¿å­˜ç»“æœåˆ°æŒ‡å®šè·¯å¾„
                    for res in result:
                        # PaddleOCR çš„ save_to_json æ–¹æ³•éœ€è¦å®Œæ•´è·¯å¾„
                        res.save_to_json(
                            os.path.join(
                                self.cache_dir, os.path.basename(region_json_path)
                            )
                        )

                    # æ›´æ–°ç¼“å­˜æ•°æ®åº“
                    self._save_to_cache_db(region_cache_path, region_json_path, regions)

            if not result or len(result) == 0:
                self.logger.warning("OCRè¯†åˆ«ç»“æœä¸ºç©º")
                return self._empty_result()

            # æ”¶é›†æ‰€æœ‰åŒ¹é…ç»“æœ
            all_matches = []
            for res in result:
                # ä½¿ç”¨å‡½æ•°çº§åˆ«çš„ç¼“å­˜å’Œè€—æ—¶ä¿¡æ¯
                res_cache_used = cache_used
                res_elapsed_time = elapsed_time
                # æ”¯æŒä¸¤ç§è®¿é—®æ–¹å¼ï¼šå±æ€§è®¿é—®å’Œå­—å…¸è®¿é—®
                if hasattr(res, "rec_texts"):
                    rec_texts = res.rec_texts
                    rec_scores = res.rec_scores
                    dt_polys = res.dt_polys
                elif isinstance(res, dict):
                    rec_texts = res.get("rec_texts", [])
                    rec_scores = res.get("rec_scores", [])
                    dt_polys = res.get("dt_polys", [])
                else:
                    # å°è¯•ä½œä¸ºå­—å…¸è®¿é—®ï¼ˆOCRResult å¯¹è±¡ï¼‰
                    try:
                        rec_texts = res["rec_texts"]
                        rec_scores = res["rec_scores"]
                        dt_polys = res["dt_polys"]
                    except (KeyError, TypeError):
                        rec_texts = []
                        rec_scores = []
                        dt_polys = []

                # æŸ¥æ‰¾åŒ¹é…çš„æ–‡å­—
                for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                    if score >= confidence_threshold and target_text in text:
                        if i < len(dt_polys):
                            poly = dt_polys[i]

                            # è°ƒæ•´åæ ‡åˆ°åŸå›¾
                            adjusted_poly = self._adjust_coordinates_to_full_image(
                                poly, offset
                            )

                            # è®¡ç®—ä¸­å¿ƒç‚¹
                            x_coords = [point[0] for point in adjusted_poly]
                            y_coords = [point[1] for point in adjusted_poly]
                            center_x = int(sum(x_coords) / len(x_coords))
                            center_y = int(sum(y_coords) / len(y_coords))

                            all_matches.append(
                                {
                                    "center": (center_x, center_y),
                                    "text": text,
                                    "confidence": score,
                                    "bbox": adjusted_poly,
                                    "index": len(all_matches) + 1,
                                    "cache_used": res_cache_used,
                                    "elapsed_time": res_elapsed_time,
                                }
                            )

            # å¤„ç†åŒ¹é…ç»“æœ
            total_matches = len(all_matches)

            if total_matches == 0:
                self.logger.debug(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡å­—: '{target_text}'")
                return self._empty_result()

            # é€‰æ‹©æŒ‡å®šçš„åŒ¹é…é¡¹
            if occurrence > total_matches:
                selected_match = all_matches[-1]
                selected_index = total_matches
            else:
                selected_match = all_matches[occurrence - 1]
                selected_index = occurrence

            # è¾“å‡ºå…³é”®ä¿¡æ¯ï¼šåªè¾“å‡ºé€‰ä¸­çš„åŒ¹é…
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº†ç¼“å­˜
            cache_used = False
            if "cache_used" in selected_match:
                cache_used = selected_match["cache_used"]

            # è®¡ç®—è€—æ—¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            elapsed_time_str = ""
            if "elapsed_time" in selected_match:
                elapsed_time_str = f" è€—æ—¶:{selected_match['elapsed_time']:.3f}s"

            # è¾“å‡ºé€‰ä¸­çš„åŒ¹é…ä¿¡æ¯
            self.logger.info(
                f"æ‰¾åˆ°æ–‡å­—: '{selected_match['text']}' "
                f"ç½®ä¿¡åº¦:{selected_match['confidence']:.3f} "
                f"ä½ç½®:{selected_match['center']} "
                f"ç¼“å­˜:{'æ˜¯' if cache_used else 'å¦'}"
                f"{elapsed_time_str}"
            )

            return {
                "found": True,
                "center": selected_match["center"],
                "text": selected_match["text"],
                "confidence": selected_match["confidence"],
                "bbox": selected_match["bbox"],
                "total_matches": total_matches,
                "selected_index": selected_index,
            }

        except Exception as e:
            self.logger.error(f"åŒºåŸŸæœç´¢å‡ºé”™: {e}")
            return self._empty_result()

    def _get_region_description(self, regions: Optional[List[int]]) -> str:
        """
        è·å–åŒºåŸŸçš„æè¿°æ–‡å­—

        Args:
            regions: åŒºåŸŸåˆ—è¡¨

        Returns:
            åŒºåŸŸæè¿°ï¼Œå¦‚ "åŒºåŸŸ[1,2,3]ï¼ˆä¸Šéƒ¨ï¼‰"
        """
        if not regions:
            return "å…¨å±"

        # åˆå¹¶åŒºåŸŸ
        min_row, max_row, min_col, max_col = self._merge_regions(regions)

        # ç”Ÿæˆæè¿°
        parts = []

        # è¡Œæè¿°
        if min_row == max_row:
            row_names = ["ä¸Šéƒ¨", "ä¸­éƒ¨", "ä¸‹éƒ¨"]
            parts.append(row_names[min_row])
        elif min_row == 0 and max_row == 2:
            parts.append("å…¨é«˜")
        else:
            parts.append(f"ç¬¬{min_row + 1}-{max_row + 1}è¡Œ")

        # åˆ—æè¿°
        if min_col == max_col:
            col_names = ["å·¦ä¾§", "ä¸­é—´", "å³ä¾§"]
            parts.append(col_names[min_col])
        elif min_col == 0 and max_col == 2:
            parts.append("å…¨å®½")
        else:
            parts.append(f"ç¬¬{min_col + 1}-{max_col + 1}åˆ—")

        region_str = ",".join(map(str, sorted(regions)))
        return f"åŒºåŸŸ[{region_str}]ï¼ˆ{' '.join(parts)}ï¼‰"

    def _empty_result(self) -> Dict[str, Any]:
        """è¿”å›ç©ºçš„æŸ¥æ‰¾ç»“æœ"""
        return {
            "found": False,
            "center": None,
            "text": None,
            "confidence": None,
            "bbox": None,
            "total_matches": 0,
            "selected_index": 0,
        }

    def capture_and_find_text(
        self,
        target_text,
        confidence_threshold=0.5,
        screenshot_path=None,
        occurrence=1,
        use_cache=True,
        regions: Optional[List[int]] = None,
    ):
        """
        æˆªå›¾å¹¶æŸ¥æ‰¾ç›®æ ‡æ–‡å­—çš„ä½ç½®

        Args:
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            screenshot_path (str, optional): æˆªå›¾ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨éšæœºä¸´æ—¶æ–‡ä»¶å
            occurrence (int): æŒ‡å®šç‚¹å‡»ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æœç´¢æ•´ä¸ªå›¾åƒ

        Returns:
            dict: åŒ…å«æŸ¥æ‰¾ç»“æœçš„å­—å…¸ï¼Œæ ¼å¼åŒfind_text_in_image
        """
        temp_screenshot_paths: Set[str] = set()

        def _create_temp_screenshot_path() -> str:
            """ç”Ÿæˆå¹¶è®°å½•ä¸´æ—¶æˆªå›¾è·¯å¾„"""
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            unique_id = str(uuid.uuid4())[:8]
            temp_path = os.path.join(
                self.temp_dir, f"screenshot_{timestamp}_{unique_id}.png"
            )
            temp_screenshot_paths.add(temp_path)
            return temp_path

        user_provided_path = screenshot_path is not None

        try:
            if not user_provided_path:
                screenshot_path = _create_temp_screenshot_path()

            # é¦–æ¬¡æˆªå›¾
            snapshot(filename=screenshot_path)
            self.logger.debug(f"æˆªå›¾ä¿å­˜åˆ°: {screenshot_path}")

            # åœ¨æˆªå›¾ä¸­æŸ¥æ‰¾æ–‡å­—
            result = self.find_text_in_image(
                screenshot_path,
                target_text,
                confidence_threshold,
                occurrence,
                use_cache,
                regions,
            )

            # å¦‚æœä½¿ç”¨ç¼“å­˜ä½†æœªæ‰¾åˆ°ï¼Œè¿›è¡Œå®æ—¶æˆªå›¾é‡è¯•
            if use_cache and (not result or not result.get("found")):
                self.logger.info("ç¼“å­˜ OCR æœªæ‰¾åˆ°ç›®æ ‡æ–‡å­—ï¼Œå°è¯•å®æ—¶æˆªå›¾é‡è¯•")
                fallback_path = (
                    screenshot_path
                    if user_provided_path
                    else _create_temp_screenshot_path()
                )

                snapshot(filename=fallback_path)
                self.logger.debug(f"å®æ—¶æˆªå›¾ä¿å­˜åˆ°: {fallback_path}")

                fallback_result = self.find_text_in_image(
                    fallback_path,
                    target_text,
                    confidence_threshold,
                    occurrence,
                    use_cache=False,
                    regions=regions,
                )

                if fallback_result:
                    result = fallback_result

                # å¦‚æœå®æ—¶è¯†åˆ«æˆåŠŸï¼Œåˆ·æ–°ç¼“å­˜
                if fallback_result and fallback_result.get("found") and regions is None:
                    base_name, _ = os.path.splitext(os.path.basename(fallback_path))
                    json_file = os.path.join(self.cache_dir, f"{base_name}_res.json")
                    if os.path.exists(json_file):
                        try:
                            self._save_to_cache(fallback_path, json_file, regions)
                            self.logger.debug("å®æ—¶æˆªå›¾ç»“æœå·²åˆ·æ–°ç¼“å­˜")
                        except Exception as cache_err:
                            self.logger.warning(f"åˆ·æ–°ç¼“å­˜å¤±è´¥: {cache_err}")

            return result

        except Exception as e:
            self.logger.error(f"æˆªå›¾å’Œè¯†åˆ«è¿‡ç¨‹å‡ºé”™: {e}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }
        finally:
            if self.delete_temp_screenshots:
                for temp_file in list(temp_screenshot_paths):
                    if not temp_file:
                        continue
                    try:
                        if os.path.exists(temp_file):
                            os.remove(temp_file)
                            self.logger.debug(f"å·²åˆ é™¤ä¸´æ—¶æˆªå›¾: {temp_file}")
                    except Exception as cleanup_error:
                        self.logger.warning(f"åˆ é™¤ä¸´æ—¶æˆªå›¾å¤±è´¥: {cleanup_error}")

    def find_and_click_text(
        self,
        target_text,
        confidence_threshold=0.5,
        screenshot_path=None,
        occurrence=1,
        use_cache=True,
        regions: Optional[List[int]] = None,
    ):
        """
        æˆªå›¾ã€æŸ¥æ‰¾æ–‡å­—å¹¶ç‚¹å‡»å…¶ä¸­å¿ƒç‚¹

        Args:
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            screenshot_path (str, optional): æˆªå›¾ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨éšæœºä¸´æ—¶æ–‡ä»¶å
            occurrence (int): æŒ‡å®šç‚¹å‡»ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼Œé»˜è®¤ä¸º True
            regions (List[int], optional): è¦æœç´¢çš„åŒºåŸŸåˆ—è¡¨ï¼ˆ1-9ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™æœç´¢æ•´ä¸ªå›¾åƒ

        Returns:
            bool: æ˜¯å¦æˆåŠŸæ‰¾åˆ°å¹¶ç‚¹å‡»
        """
        result = self.capture_and_find_text(
            target_text,
            confidence_threshold,
            screenshot_path,
            occurrence,
            use_cache,
            regions,
        )

        if result["found"]:
            center = result["center"]
            self.logger.debug(f"ç‚¹å‡»ä½ç½®: {center}")
            touch(center)
            return True
        else:
            self.logger.warning(f"æ— æ³•ç‚¹å‡»ï¼Œæœªæ‰¾åˆ°æ–‡å­—: '{target_text}'")
            return False

    def _find_text_in_json(
        self, json_file_path, target_text, confidence_threshold=0.5, occurrence=1
    ):
        """
        ä»PaddleOCRè¾“å‡ºçš„JSONæ–‡ä»¶ä¸­æŸ¥æ‰¾ç›®æ ‡æ–‡å­—

        Args:
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)
            occurrence (int): æŒ‡å®šè¿”å›ç¬¬å‡ ä¸ªå‡ºç°çš„æ–‡å­— (1-based)ï¼Œé»˜è®¤ä¸º1

        Returns:
            dict: æŸ¥æ‰¾ç»“æœå­—å…¸
        """
        try:
            # è¯»å–JSONæ–‡ä»¶
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # è·å–è¯†åˆ«çš„æ–‡å­—åˆ—è¡¨å’Œå¯¹åº”çš„åæ ‡æ¡†
            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])  # æ£€æµ‹æ¡†åæ ‡

            self.logger.debug(f"åœ¨JSONä¸­æŸ¥æ‰¾æ–‡å­—: '{target_text}' (ç¬¬{occurrence}ä¸ª)")
            self.logger.debug(f"æ€»å…±è¯†åˆ«åˆ° {len(rec_texts)} ä¸ªæ–‡å­—")

            # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„æ–‡å­—
            matches = []
            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                self.logger.debug(f"  {i + 1:2d}. '{text}' (ç½®ä¿¡åº¦: {score:.3f})")

                # æ£€æŸ¥ç½®ä¿¡åº¦å’Œæ–‡å­—åŒ¹é…ï¼ˆè¯†åˆ«å‡ºçš„æ–‡å­—åŒ…å«ç›®æ ‡æ–‡å­—ï¼‰
                if score >= confidence_threshold and target_text in text:
                    # è·å–å¯¹åº”çš„åæ ‡æ¡†
                    if i < len(dt_polys):
                        poly = dt_polys[i]

                        # è®¡ç®—ä¸­å¿ƒç‚¹åæ ‡
                        # poly æ ¼å¼: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        center_x = int(sum(x_coords) / len(x_coords))
                        center_y = int(sum(y_coords) / len(y_coords))

                        matches.append(
                            {
                                "center": (center_x, center_y),
                                "text": text,
                                "confidence": score,
                                "bbox": poly,
                                "index": len(matches) + 1,
                            }
                        )

            total_matches = len(matches)
            # ä¸åœ¨è¿™é‡Œè¾“å‡ºåŒ¹é…æ•°é‡ï¼Œæ”¹ä¸ºåœ¨é€‰æ‹©åè¾“å‡º

            if total_matches == 0:
                self.logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡å­—: '{target_text}'")
                return {
                    "found": False,
                    "center": None,
                    "text": None,
                    "confidence": None,
                    "bbox": None,
                    "total_matches": 0,
                    "selected_index": 0,
                }

            # æ˜¾ç¤ºæ‰€æœ‰åŒ¹é…çš„æ–‡å­—
            # ä¸å†è¾“å‡ºæ‰€æœ‰åŒ¹é…ï¼Œåªè¾“å‡ºé€‰ä¸­çš„

            # é€‰æ‹©æŒ‡å®šçš„åŒ¹é…é¡¹
            if occurrence > total_matches:
                self.logger.warning(
                    f"è¯·æ±‚ç¬¬{occurrence}ä¸ªåŒ¹é…ï¼Œä½†åªæ‰¾åˆ°{total_matches}ä¸ªï¼Œä½¿ç”¨æœ€åä¸€ä¸ª"
                )
                selected_match = matches[-1]
                selected_index = total_matches
            else:
                selected_match = matches[occurrence - 1]
                selected_index = occurrence

            # è¾“å‡ºé€‰ä¸­çš„åŒ¹é…ä¿¡æ¯
            self.logger.info(
                f"æ‰¾åˆ°æ–‡å­—: '{selected_match['text']}' "
                f"ç½®ä¿¡åº¦:{selected_match['confidence']:.3f} "
                f"ä½ç½®:{selected_match['center']} "
                f"ç¼“å­˜:æ˜¯"
            )

            return {
                "found": True,
                "center": selected_match["center"],
                "text": selected_match["text"],
                "confidence": selected_match["confidence"],
                "bbox": selected_match["bbox"],
                "total_matches": total_matches,
                "selected_index": selected_index,
            }

        except FileNotFoundError:
            self.logger.error(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file_path}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }
        except json.JSONDecodeError:
            self.logger.error(f"JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {json_file_path}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }
        except Exception as e:
            self.logger.error(f"å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return {
                "found": False,
                "center": None,
                "text": None,
                "confidence": None,
                "bbox": None,
                "total_matches": 0,
                "selected_index": 0,
            }

    def find_all_matching_texts(
        self, image_path, target_text, confidence_threshold=0.5
    ):
        """
        æŸ¥æ‰¾å›¾åƒä¸­æ‰€æœ‰åŒ¹é…çš„æ–‡å­—

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)

        Returns:
            list: åŒ…å«æ‰€æœ‰åŒ¹é…æ–‡å­—ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«center, text, confidence, bbox
        """
        try:
            # OCR è¯†åˆ«
            result = self._predict_with_timing(image_path)

            if not result or len(result) == 0:
                self.logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return []

            # ä¿å­˜è¯†åˆ«ç»“æœåˆ°JSON
            for res in result:
                res.save_to_json(self.output_dir)

            # ä»ç»“æœä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡å­—
            json_file = os.path.join(
                self.output_dir,
                os.path.basename(image_path).replace(".png", "_res.json"),
            )
            return self._find_all_matching_texts_in_json(
                json_file, target_text, confidence_threshold
            )

        except Exception as e:
            self.logger.error(f"æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…æ–‡å­—æ—¶å‡ºé”™: {e}")
            return []

    def _find_all_matching_texts_in_json(
        self, json_file_path, target_text, confidence_threshold=0.5
    ):
        """
        ä»JSONæ–‡ä»¶ä¸­æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„æ–‡å­—

        Args:
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„
            target_text (str): è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡å­—
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ (0-1)

        Returns:
            list: æ‰€æœ‰åŒ¹é…çš„æ–‡å­—ä¿¡æ¯åˆ—è¡¨
        """
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])

            matches = []
            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                # æ£€æŸ¥ç½®ä¿¡åº¦å’Œæ–‡å­—åŒ¹é…ï¼ˆè¯†åˆ«å‡ºçš„æ–‡å­—åŒ…å«ç›®æ ‡æ–‡å­—ï¼‰
                if score >= confidence_threshold and target_text in text:
                    if i < len(dt_polys):
                        poly = dt_polys[i]
                        x_coords = [point[0] for point in poly]
                        y_coords = [point[1] for point in poly]
                        center_x = int(sum(x_coords) / len(x_coords))
                        center_y = int(sum(y_coords) / len(y_coords))

                        matches.append(
                            {
                                "center": (center_x, center_y),
                                "text": text,
                                "confidence": score,
                                "bbox": poly,
                                "index": len(matches) + 1,
                            }
                        )

            return matches

        except Exception as e:
            self.logger.error(f"å¤„ç†JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return []

    def get_all_texts_from_image(self, image_path):
        """
        è·å–å›¾åƒä¸­æ‰€æœ‰è¯†åˆ«åˆ°çš„æ–‡å­—ä¿¡æ¯

        Args:
            image_path (str): å›¾åƒæ–‡ä»¶è·¯å¾„

        Returns:
            list: åŒ…å«æ‰€æœ‰æ–‡å­—ä¿¡æ¯çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå­—å…¸åŒ…å«text, confidence, center, bbox
        """
        try:
            # OCR è¯†åˆ«
            result = self._predict_with_timing(image_path)

            if not result or len(result) == 0:
                self.logger.warning(f"OCRè¯†åˆ«ç»“æœä¸ºç©º: {image_path}")
                return []

            # ä¿å­˜è¯†åˆ«ç»“æœåˆ°JSON
            for res in result:
                res.save_to_json(self.output_dir)

            # ä»JSONæ–‡ä»¶è¯»å–æ‰€æœ‰æ–‡å­—
            json_file = os.path.join(
                self.output_dir,
                os.path.basename(image_path).replace(".png", "_res.json"),
            )
            return self._get_all_texts_from_json(json_file)

        except Exception as e:
            self.logger.error(f"è·å–å›¾åƒæ–‡å­—ä¿¡æ¯å‡ºé”™: {e}")
            return []

    def _get_all_texts_from_json(self, json_file_path):
        """
        ä»JSONæ–‡ä»¶ä¸­è·å–æ‰€æœ‰æ–‡å­—ä¿¡æ¯

        Args:
            json_file_path (str): JSONæ–‡ä»¶è·¯å¾„

        Returns:
            list: æ‰€æœ‰æ–‡å­—ä¿¡æ¯åˆ—è¡¨
        """
        try:
            with open(json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            rec_texts = data.get("rec_texts", [])
            rec_scores = data.get("rec_scores", [])
            dt_polys = data.get("dt_polys", [])

            texts_info = []

            for i, (text, score) in enumerate(zip(rec_texts, rec_scores)):
                if i < len(dt_polys):
                    poly = dt_polys[i]
                    x_coords = [point[0] for point in poly]
                    y_coords = [point[1] for point in poly]
                    center_x = int(sum(x_coords) / len(x_coords))
                    center_y = int(sum(y_coords) / len(y_coords))

                    texts_info.append(
                        {
                            "text": text,
                            "confidence": score,
                            "center": (center_x, center_y),
                            "bbox": poly,
                        }
                    )

            return texts_info

        except Exception as e:
            self.logger.error(f"è¯»å–JSONæ–‡ä»¶å‡ºé”™: {e}")
            return []


# æ‰¹é‡ä¸ºå…³é”®æ–¹æ³•åº”ç”¨æ—¥å¿—åˆ‡é¢
try:
    apply_logging_slice(
        [
            (OCRHelper, "capture_and_find_text"),
            (OCRHelper, "find_text_in_image"),
            (OCRHelper, "find_and_click_text"),
            (OCRHelper, "_get_or_create_ocr_result"),
            (OCRHelper, "_find_text_in_regions"),
            (OCRHelper, "_predict_with_timing"),
        ],
        level="DEBUG",
    )
except Exception:
    pass
